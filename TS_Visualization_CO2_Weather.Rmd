---
title: Time Series Data Analysis -
subtitle: Atmospheric $CO_2$ Concentration, Weather Temperature or Precipitation
author: "Wolfgang Vollmer"
date: '`r Sys.Date()`'
output:
  pdf_document:
    fig_caption: no
    fig_height: 6
    fig_width: 12
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document: default
  word_document:
    toc: yes
  odt_document:
urlcolor: blue
papersize: a4
params:
  city: "Basel"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE,      
                      message = FALSE, fig.width = 7, fig.asp = 0.618)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
)
```


```{r initialization, include = FALSE}
################################################################################
## Plot and Analyze                                                           ##
## Yearly/Monthly CO2 or Weather Temperature / Precipitation Data             ##
##                                                                            ##
##                                                Wolfgang Vollmer, Feb 2020  ##
##                                                                            ##
################################################################################

######  variables clean up, library() and	source() statements
# setwd("D:/Wolfgang/Programs-R/R-WeatherAnalysis")
setwd("D:/Wolfgang/Programs-R/R-TimeSeriesAnalysis")
rm(list=ls()) # deletes all existing objects / variables !!
Prog.Start <- Sys.time()

suppressMessages(library(tidyverse))
suppressMessages(library(magrittr))
library(lubridate)
library(fpp3)  # tsibble, tsibbledata, fable, and feasts packages & some tidyverse
library(stlplus) # Enhanced Seasonal Decomposition of Time Series by Loess
library(DT)      # R interface to the JavaScript library DataTables
library(rlang)

# library(grid)      # load package for function to create multi-plot setup

######  Plot and util functions
source("./uts_TimeSeries.R")  # utility functions for time series
source("./ggts_TimeSeries.R") # ggplot2 functions for time series plots

# for mean year values already NA values not stripped (na.rm = FALSE) 
# => for mean values over some years / periods stripping is not needed
# => valid for monthly and yearly data

fable_plot <- FALSE # plot also additional plots generated with fabletools

test_eval <- TRUE

```

```{r settings, eval = TRUE, include = FALSE}

city_list <- c("Cottbus", "Giessen", "Hohenpeissenberg", "Mannheim", "Potsdam",
               "Basel", "Davos", "England", "Mauna Loa", "Basel_Giessen")
city <- c("Mannheim")
city %in% city_list 
n_mismatches <- sum(ifelse(city %in% city_list, 0, 1))
if (n_mismatches > 0) { 
  abort(paste(paste(city, collapse = ", "), 
              "does not match to defined list of allowed cities:", 
              paste(city_list, collapse = ", ")))
}
if (length(city) > 1) { 
  abort(paste(paste(city, collapse = ", "), 
              "- support of more then one city entry not yet implemented")) 
}

# city <- params$city # removed by rm(list=ls())

###### dafault settings for CO2
###### for Weather Temperature/Precipitation to be adjusted 
freq <- 12          # season lenght: year with 12 months
span <- 7           # width of the rolling window in years, Wheater: => 30
span_m <- span * 12 # width of the rolling window in months

title_rmd <- "Atmospheric Carbon Dioxide - Mauna Loa"
topic <- "Mauna Loa CO2 Concentrations"
y_label <- expression(paste(CO[2], " Concentration / ppm"))
# for Times Series decompsition with stlplus()
season_window <- 7     # Temperature:  s.window=31, t.window=19
trend_window  <- 41

# key <- c("City", "Measure")   # all data have to have "City" & "Measure" column
key <- c("Measure") # w/o City reduces "strip" naming w/ facet_wrap/grid

# params checked only for city != "Mauna Loa"              
temp_and_precip <- TRUE # if both, Temperature & Precipitation to be analyzed
# param checked only for temp_and_precip == FALSE
temperature <- TRUE    # TRUE/FALSE: Temperature / Precipitation to be analyzed

y_label_measure <- c(Temperature = expression(paste("Temperature / ", degree*C)),
                     Precipitation = "Precipitation / mm/Month",
                     CO2 = expression(paste(CO[2], " Concentration / ppm")))

```

```{r adapt settings, eval = TRUE, include = FALSE}

if (city == "Mauna Loa") {
  overwrite <- FALSE  # replace orig data w/ data start 1959 Jan instead 1958 Mar
  # and w/o NAs since interpolated data column is then used
  measure <- "CO2"
} else if (city %in% c("Cottbus", "Giessen", "Mannheim", "Basel", "Basel_Giessen")) {
  span <- 30           # width of the rolling window in years, Wheater: => 30
  season_window <- 31  # seasonal structure long running  # Precip to be checked
  trend_window <- 19   # to allow more rapid changes
  if (temp_and_precip) {
    title_rmd <- paste(city, " - Weather Temperature and Precipitation Data")
    topic <- paste(city, "- Temperature and Precipitation")
    
    y_label <- expression(paste("Temperature / ", degree*C, 
                                " rsp. Precipitation / mm/Month"))    
    measure <- c("Temperature and Precipitation")
  } else {
    if (temperature) {
      measure <- "Temperature"
      y_label <- expression(paste("Temperature / ", degree*C))
    } else {
      measure <- "Precipitation"
      y_label <- "Precipitation / mm/Month"
    }
    title_rmd <- paste(city, " - Weather", measure, "Data")
    topic <- paste(city, "-", measure)
  }
} else {
  abort("City ", city, " not valid, no *.rds data file exists !!")
}  

```


# Visualization of `r title_rmd`

```{r rds orig data reading, eval = TRUE}
if (city == "Mauna Loa") {
#            decimal     average   interpolated    trend    #days
#             date                             (season corr)
# 1958   3    1958.208      315.71      315.71      314.62     -1
# 1958   4    1958.292      317.45      317.45      315.29     -1
header <- FALSE  # header line with column names exist
sep <- " " # field separator in txt file = blank
dec <- "."
skip <- 72 # skip no line, start with header line and succeeding data
na_sign <- "-99.99"  # NA value indicator in data source
file <- "./Data_Mauna_Loa_CO2/co2_mm_mlo.txt"
# verify if data file exists
if(!file.exists(file))
  stop("Data file ", file, " not found!")

mauna_loa_co2 <- read_table(file, col_names = header, na = na_sign, skip = skip)
names(mauna_loa_co2) <- c("Year", "Month", "year.month", "average",  
                          "interpolated", "trend", "#days")
# "average" column:      contains the monthly mean; with NA = -99.99
# "interpolated" column: average values with interpolation for NAs
#                        by 7-year seasonal window around each monthly value
# "trend" column: value for each month by removing the seasonal cycle
#  Year Month year.month average interpolated `trend (season corr)`
#  1958     3   1958.208 	315.71	     315.71	      314.62
#  :
#  2019  	 12	  2019.958	411.76	     411.76	      412.43

## monthly data - check for gaps and fill w/ NAs as far as needed
data_monthly <- mauna_loa_co2 %>% 
  mutate(Month = factor(Month))
levels(data_monthly$Month) <- month.abb # first Month 3 -> Mar correctly assigned
         
data_monthly <- uts_data_check_and_fill_w_na(
  dplyr::select(data_monthly, Year, Month, interpolated)) %>% 
  rename(count = interpolated)  %>% 
  mutate(City = city,
         Measure = "CO2")

## monthly data wide 
## - adds automatically NAs for months before first month in first year and
## - adds automatically NAs for months after  last  month in last year
data_monthly_wide <- data_monthly  %>% 
  pivot_wider(id_cols = c(Year, City, Measure), 
              names_from = Month, 
              values_from = count) %>% 
  dplyr::select(Year, City, Measure, month.abb) 
# to get the right ordering Jan, Feb, ..., Dec

# data_monthly - store with 'completed' first year, now with NA for Jan and Feb
data_monthly <- data_monthly_wide %>%
  pivot_longer(cols = Jan:Dec, # cols = "1":"12", #
               names_to = c("Month"),
               values_to = "count")  %>%
  unite(Year_Month, Year, Month, sep = "-", remove = FALSE) %>% 
  # => keep Year, Month column
  mutate(Year_Month = yearmonth(Year_Month)) %>% 
  as_tsibble(index = Year_Month, key = key)
}

```



```{r rds data reading, eval = FALSE}
##  Mauna Loa overwrite - data reading

if (city == "Mauna Loa" & overwrite) {
  data_monthly_wide <- readRDS("./Data_Mauna_Loa_CO2/CO2_Mauna_Loa.rds") %>% 
    mutate(City = city,
           Measure = "CO2")
  
  data_monthly <- data_monthly_wide %>% 
        pivot_longer(cols = Jan:Dec,
                 names_to = c("Month"),
                 values_to = "count") %>% 
    dplyr::select(Year, Month, count, City, Measure)
  data_monthly <- uts_data_check_and_fill_w_na(data_monthly, key = key)
}

```

```{r weather test data, eval = TRUE}

weather_test_w_na <- FALSE  # for test purposes only - filter data from 1940 - 1954
                            # with missing year 1949 and lot of NAs

if (city != "Mauna Loa" & city != "Basel_Giessen") {
  # data_monthly_wide <- 
  #   readRDS(paste0("../R-WeatherAnalysis/WeatherData_", city, ".rds")) # %>% 
  # # read txt file with csv data format
  file <- paste0("D:/Wolfgang/Programs-R/R-TimeSeriesAnalysis",
                 "/R-WeatherAnalysis/Data_Weather/Weather-Data-Input_", 
                 city, ".csv")
  data_monthly_wide <- read_csv(file, col_names = TRUE, skip = 0)

  if (!temp_and_precip) {
    # Measure <- NULL
    data_monthly_wide %<>% filter(Measure == measure)
  }
  if (weather_test_w_na) {
    test_monthly_wide <- filter(data_monthly_wide, Year >= 1940 & Year <= 1954 
                                & Year & Year != 1949)
    # note:   & Year & Year != 1949 not feasible, otherwise ts() indexing wrong
    #   running from 1940 - 1953 instead 1954 if 1949 is taken out
    test_monthly_wide[1, 9:11] <- NA
    test_monthly_wide[2, 6:7] <- NA
    test_monthly_wide[2, 12] <- NA
    
    data_monthly_wide <- test_monthly_wide
  }
  data_monthly <- data_monthly_wide %>% 
    pivot_longer(cols = Jan:Dec,
                 names_to = c("Month"),
                 values_to = "count") %>% 
    dplyr::select(Year, Month, count, City, Measure)
  
  data_monthly <- uts_data_check_and_fill_w_na(data_monthly, key = key, 
                                               add_precip = FALSE) 
  # %>% dplyr::select(Year_Month, count)
}

if (city == "Basel_Giessen") {
  source("Gen_Data_Basel_Giessen.R")
  data_monthly <- gen_data_basel_giessen()
  
  data_monthly_wide <-  as_tibble(data_monthly) %>% 
              pivot_wider(id_cols = c(Year, City, Measure), 
                         names_from = Month, 
                         values_from = count)

 # mutate(Month = format(ISOdate(2000, Month, 1, tz = "GMT"), "%b")) %>%  
 # provides German spelling Mrz, Mai, .., Dez
 # 
 # names(data_monthly_wide) <- c("Year", "Measure", "City",  month.abb)
}
```

```{r missing data, eval = TRUE}
# get common data structure for plots w/ & w/o replaced counts 
data_monthly %<>% mutate(Raw = count,
                         Interpolated = NA,
                         NA_replace = NA)

## are there any NAs in data_monthly$count
n_row <- nrow(data_monthly)
n_na <- as_tibble(data_monthly) %>% 
  summarise(nr_na = sum(is.na(count))) %>% sum()
## => replace NA by interpolation separate for each time series
if (n_na != 0) {
  warning(paste(city, measure, "data file has missing values, #NA=",
                n_na, "out of", n_row))
  data_monthly_new <- tibble()   
  for (i in unique(key_data(data_monthly)[[1]])) {    
   # for (j in unique(key_data(data_monthly)[[2]])) {
      # data <- filter(data_monthly, City == i & Measure == j)
      data <- filter(data_monthly, Measure == i)
      data_interpolate <- as_tibble(uts_interpolate_na(data, freq))
      data_monthly_new <- bind_rows(data_interpolate, data_monthly_new)
    }
    #  NAs in column 'count' are now replaced by interpolated values !!
    #    previous NAs are in RAW (old count) and 
    #    replaced NAs are in updated count (only for easy check)
    # yearmonth() class is lost by bind_row
    
  }      
  data_monthly <- data_monthly_new %>% 
    mutate(Year_Month = yearmonth(Year_Month)) %>%  
    as_tsibble(index = Year_Month, key = key)
# } 

data_monthly %<>%  group_by(City, Measure) 
data_monthly$Month <- factor(data_monthly$Month, levels = month.abb)
data_monthly$Measure <- factor(data_monthly$Measure,
                               levels = c("Temperature", "Precipitation", "CO2"))

# saveRDS(data_monthly, paste0("test_data_monthly_", city, ".rds"))  
```


```{r provide yearly and monthly means}

################### group_by() => index_by(time_index-group) ###################################
# index_by() is the counterpart of group_by() in temporal context, 
# but it only groups the time index => afterwards group_by_key()

# first / last year - overall (w/o grouping) 
first_year <- data_monthly %$% year(min((Year_Month))) 
last_year <- data_monthly %$% year(max((Year_Month)))

# first / last year - for each group
first_last_year <- 
  as_tibble(data_monthly) %>% 
  group_by(City, Measure) %>% 
  summarise(mean = mean(count), n = n(),
            first_year = min(Year),
            last_year = max(Year))
# first_last_year

# mean of month over all years grouped by (City, Measure, Month)
mean_monthly <- data_monthly %>%  
  index_by(Month = ~ month(.)) %>%  
  as_tibble() %>%   
  mutate(Month = factor(Month)) %>% 
  group_by(City, Measure, Month) %>% 
  summarise(Month_avg = mean(count))
levels(mean_monthly$Month) <- month.abb  # rename levels 1->Jan, ...
# slice(mean_monthly, 1:6, Measure)

data_yearly <- uts_gen_yearly_seasonal_avg(data_monthly)
data_yearly <- as_tsibble(data_yearly, index = Year, key = key)
# slice(data_yearly, 1:6)
# saveRDS(data_yearly, paste0("test_data_yearly_", city, ".rds"))
```


## Monthly `r topic` - `r first_year`-`r last_year`

### Time Plot with Rolling Mean


```{r time plot outlining replaced NAs, eval = TRUE, fig.width= 7, fig.height= 4}
if (as_tibble(data_monthly) %>% summarise(nr_na = sum(!is.na(NA_replace))) > 0) 
  { 
  # only useful and working if at least on data_monthly$NA_replace != NA
  # other helper: data_monthly$NA_replace[1] <- data_monthly$Raw[1]
# group_by() required, to avoid "continous" roll_mean (over group limit)
data_plot <- as_tibble(data_monthly) %>% 
  group_by(City, Measure)

plot_monthly <- 
  ggts_w_rol_mean(data_plot, Year_Month, y = Raw, span = span_m) +
  geom_point(aes(y = data_monthly$NA_replace), col = "green", size = 0.8) +
      facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left") +
  labs(y = y_label) +
  ggtitle(paste(topic, "(green: interpolated values)")) 
plot_monthly
}
```

```{r time plot with fabletools, eval = FALSE, fig.width= 7, fig.height= 4}

data_monthly %>% autoplot(count) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free") +
  labs(y = y_label) +
  ggtitle(paste(topic, "(green: interpolated values)")) 
  
```

```{r Plot Monthly with Rolling Mean, eval = TRUE, fig.width= 7, fig.height= 4}

# group_by() required, to avoid "continous" roll_mean (over group limit)
# rol_mean over whole time series needed for second plot otherwise shorter
data_plot <- as_tibble(data_monthly) %>% 
  group_by(City, Measure) %>% 
  mutate(rol_mean_long = stats::filter(count, filter = rep(1/span_m, span_m)))

plot_monthly <- 
  ggts_w_rol_mean(data_plot, Year_Month, count, span = span_m) +
  labs(y = y_label,
       title = paste(topic, " - Time Plot of Monthly Data"))+
      facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left")
# = facet_wrap(vars(Measure, City), ncol = 1, scales = "free")
plot_monthly 


years <- 15

past_years <- TRUE  # tricky, since roll mean for early years to be kept
if (past_years) {
 x_min <- data_plot %$% max(Year_Month) - freq * years + 1 
                                            # yearmonth(last_year - years + 1)
 x_max <- data_plot %$% max(Year_Month)         # max(data_monthly$Year_Month)
} else {     # take first ten years
  x_min <- data_plot %$% min(Year_Month) # yearmonth(first_year)              
  x_max <- data_plot %$% min(Year_Month) + freq * years - 1 # data_monthly$Year_Month[years*12]
}

# y_min/y_max same value for Temeperature & Percipitation => not useful to be taken
y_min <- data_plot %$% min(count, na.rm = TRUE)
y_max <- data_plot %$%  max(count, na.rm = TRUE)

# mauna_co2_plot %+% plot_data not running: rol_mean missing 
# => to be added before filtering
data_plot %<>% filter(Year_Month >= x_min &  Year_Month <= x_max)
plot_monthly <-  # ggplot_w_rol_mean( , x = "")
  ggts_w_rol_mean(data_plot, Year_Month, count, span = span_m) +
  # geom_point() +
  labs(y = y_label,
       title = paste(topic, " - Time Plot of Monthly Data")) +
      facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left")
plot_monthly + geom_line(aes(y = rol_mean_long), col = "red", size = 1, na.rm = TRUE)
                             
```


## Seasonal Plots - Monthly `r topic`

```{r monthly plot, eval = TRUE, fig.width= 7, fig.height= 7}

##  Plot Monthly data with smooth per month over Years
ggts_season_w_smooth(data = data_monthly) + 
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left") +
  # theme(legend.position = "bottom") +
  labs(y = y_label, 
       subtitle = paste(city, first_year, " - ", last_year))

```


```{r Period, eval = TRUE}

# subsume data into periods 

# calculate data mean / span years
# na.rm = FALSE / TRUE => NA not stripped (default) / NA stripped
########### Plot Period Temperature Means over the year  

# assign years to periods with length = span (e.g. 15years) 
#                                starting backwards with last full year
#                                
# period_adapt <- last_full_year %% span
# 
# span <- 7 # already at top

period_adapt <- last_year %% span  # last_year mod x years => 
# last period has to be a full period, first period may be shorter 
# assign Years to Periods w/ length of span and last full period until last year 

data_monthly_over_period  <- as_tibble(data_monthly) %>% 
  mutate(Period = span*ceiling((Year - period_adapt)/span) + period_adapt) %>%
  # assign Year_Month to Periods w/ length of span with full period with last_year 
  group_by(City, Measure, Month, Period) %>% 
  dplyr::summarise(count = mean(count, na.rm = TRUE)) %>% 
  ungroup() 

```

### Plot Monthly Variations - Cartesian and Polar Coordinates

```{r seasonal, eval = TRUE, fig.width= 7, fig.height= 4}
end_first_period <- min(data_monthly_over_period$Period)
start_last_period <- max(data_monthly_over_period$Period) - span + 1

data_monthly_over_period %<>% group_by(Month) %>% 
  mutate(count_minus_first = count - count[1]) 
  
data <- data_monthly_over_period 
col_scheme <- "YlOrRd"

period_years <- c(paste(first_year, "-", end_first_period, sep = ""),
          paste(start_last_period - 30, "-", start_last_period -1, sep = ""),
          paste(start_last_period, "-", last_year, sep = ""))
names(period_years) <- c("first", "ref", "last")

title <- paste0("Monthly Variations of ", span, "-Year Periods")
# subtitle <- paste0("First Period: ", first_year, "-", end_first_period,
#                   " -- Last Period: ", start_last_period, "-", last_year)
subtitle <- paste("First / Reference / Last Period:",
                        paste(period_years, collapse = " / "))
      
y_label_delta <- 
  expression(paste("Delta ", CO[2], " Concentration / ppm to First Period"))

plot_monthly <- ggts_year_over_month(data_monthly_over_period, period = Period) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left") +
  labs(y = y_label, title = title, subtitle = subtitle)
plot_monthly

# plot_monthly + coord_polar("x", start = pi) + # pi => rotate by 180 degrees
#   facet_wrap(vars(!!!key(data_monthly)), ncol = 1) 

# !!  coord_polar doesn't support free scales => generate separate plots
# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
      data_filter <- filter(data_monthly_over_period, Measure == i)
      plot_monthly <- ggts_year_over_month(data_filter, period = Period) +
        labs(y = y_label, title = paste(title, "-", i), subtitle = subtitle)

      print(plot_monthly + coord_polar("x", start = pi))  # pi => rotate by 180 degrees
    }
  # }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```

### Plot Monthly Variations to First Period - Cartesian and Polar Coordinates

```{r seasonal adjusted, fig.width= 7, fig.height= 4}
plot_monthly <- ggts_year_over_month(data_monthly_over_period, period = Period,
                                     y = count_minus_first) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left") +
  labs(y = "Delta to First Period", title = title, subtitle = subtitle) +
  labs(y = y_label, title = paste0(title, " - Delta to", 
                                   end_first_period, "-", end_first_period), 
       subtitle = subtitle)
plot_monthly

# !!  coord_polar doesn't support free scales => generate separate plots
# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
      data_filter <- filter(data_monthly_over_period, Measure == i)
      plot_monthly <- ggts_year_over_month(data_filter, period = Period,
                                           y = count_minus_first) +
        labs(y = "Delta to First Period", title = title, subtitle = subtitle) +
        labs(y = y_label, title = paste0(title, " - Delta to", 
                                         end_first_period + 1, "-", end_first_period + 31), 
             subtitle = subtitle)
      
      print(plot_monthly + coord_polar("x", start = pi))  # pi => rotate by 180 degrees
    }
#  }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```

```{r seasonal plots fabletools, eval = TRUE, fig.width= 7, fig.height= 4}

data_monthly %>% gg_season(count, labels = "none") +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free", 
             strip.position = "left") +
  labs(x = "Month", y = y_label,
       title = paste("Yearly Seasonal Plots - Monthly", topic))


data_monthly %>%
  gg_subseries(count) + 
  labs(x = "Month", y = y_label,
       title = paste("Monthly Subseries Plots - ", topic))

```


## Yearly  `r topic`

### Plot Yearly  `r measure`


```{r yearly, eval = TRUE, fig.width= 7, fig.height= 5}

# plot mean temperature & precipitation with running/rolling mean

## Multiply "Precipitation" values/month by 12 to get mm/Year

#  England data and running mean: The red line is a 21-point binomial filter, 
#                      which is roughly equivalent to a 10-year running mean.
# for Global Temperature Circle see:
# https://www.metoffice.gov.uk/weather/climate-change/what-is-climate-change
# 
# span of years of moving/rolling average/mean (einfacher Gleitender Durchschnitt) 
# default: method = "convolution", sides = 2 => centred) 

# Gauss-Filterung ; Filtergewichte proportional zur Gauss’schen Normalverteilung
# default: method = "convolution", sides = 2 => centred, circular = FALSE)
gauss <- function(x, sig=1) { 1/sqrt(2*pi)/sig * exp(-(x^2)/2/sig) }
x <- seq(from = -(span-1)/2, to = (span-1)/2, by = 1)
# x <- seq(from = -span, to = span, by = 1) 
sig <- span
filter_coef <- gauss(x, sig)/sum(gauss(x, sig))  # normalization 
# sum(filter_coef)  # has to be 1

# Hanning-Window/Von-Hann-Fenster Filter Paket: ‘e1071’ 
# similar gauss/bell-shaped curve
# see also:  https://de.wikipedia.org/wiki/Fensterfunktion
# plot(filter_coef, col = "red")  # gauss
# library(e1071)
# filter_coef <- hanning.window(span)/sum(hanning.window(span))
# points(hanning.window(span)/sum(hanning.window(span)), col = "blue")

# rol_mean will be calculated by ggts_w_rol_mean(); gauss() not required
# yearly_plot_data <- data_yearly %>%   
#   mutate(rol_mean = 
#            stats::filter(Year_avg, filter = rep(1/span, span)),
#          rol_gauss = 
#            stats::filter(Year_avg, filter = filter_coef)) %>% 
#   ungroup()
# 
# yearly_plot_data

data_yearly_tbl <- as_tibble(data_yearly) %>% group_by(Measure, City) 
                 # to avoid rolling mean over Precip - Temp
plot_yearly <- ggts_w_rol_mean(data_yearly_tbl, Year, Year_avg, span = span) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free",
              strip.position = "left")

plot_yearly +
  labs(y = y_label) +
  # ggtitle("Mauna Loa - Time Plot of Yearly CO2 Concentration")
  ggtitle(topic)


# * Blue:    Annual Mean Temperature / Precipitation
# * Red:     Rolling mean with span of `r span` years
# * Orange:  Rolling gaussian filtering with span of `r span` years
# * Green:   Linear Regression line
# * Darkblue: Local Polynomial Regression Fitting (Loess)
# * Black:    Reference Period `r period_years["ref"]`: Annual Mean Temperature / Precipitation

```

### Plot Seasonal Yearly `r measure`

```{r season, eval = TRUE, fig.width= 7, fig.height= 8}
# 1)  seasonal values are calculated from the monthly data by averaging the three monthly values
# 2)  Seasons are Dec-Feb (DJF), Mar-May (MAM), June-Aug (JJA), Sept-Nov (SON).
# 3)  Winter: year refers to January.
# 4)  Note for  ENGLAND TEMPERATURE data:  Seasonal data exist for Spring 1659 onwards..

ggts_season(data = data_yearly) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left") +
  labs(y = y_label)

```
`

# Analysis

## Time Series Decomposition - Trend and Seasonal Components

An *additive model* would be used when the variations around the trend do not vary
with the level of the time series whereas a *multiplicative model* would be
appropriate if the trend is proportional to the level of the time series.

Time series using an

* additive model: $y_t = T_t + C_t + S_t + \epsilon_t$  

* multiplicative model: $y_t = T_t * C_t * S_t * \epsilon_t$ 

Trend / Cycle / Seasonal / Noise component  
Cyclical components is often grouped into the Trend component

For *Seasonal decomposition of time series by Loess (stlplus)* uses in general
an additive error modle, it only provides facilities for additive decompositions.
It is possible to obtain a multiplicative decomposition by first taking logs of 
the data.

```{r decomposition replace NAs, eval = TRUE, fig.width= 7, fig.height= 10}
## decomposition

# filter(data_monthly, Measure == "Temperature")
# filter(data_monthly, Measure == "Precipitation")

# first_year <- data_monthly %$% min(year(Year_Month))
# last_year <- data_monthly %$% max(year(Year_Month))
# data_monthly <- data_monthly %>%  dplyr::select(Year_Month, count)



# only one single time series for ts() and stl() allowed

#   data_ts <- data_monthly %$% ts(count, start=c(first_year, 1), frequency = freq)
#   # t.window default: nextodd of
#   # s.window <- c(5, 7, 13, 21, 31, 61, 121, 241, 1001, 2001)
#   # (ceiling((1.5*freq) / (1-(1.5/s.window))))
#   #    s.window:   5, 7, 13, 21, 31, 61, 121    
#   # => t.window:  27 23, 21, 21, 19, 19,  19  #
#   #                        min = 19 da ceiling(1.5*12)=18 => nextodd = 19
   

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
      data_filter <- filter(data_monthly_over_period, Measure == i)
    data_monthly_new <- tibble()  
      data_ts <- filter(data_monthly, Measure == i) %$% 
        ts(count, start=c(first_year, 1), frequency = freq)
      stlplus_data <- stlplus(data_ts, s.window=season_window, t.window=trend_window)
      
      data_monthly_stlplus <- uts_stlplus_as_tibble(stlplus_data)  %>% 
        mutate(Measure = i)
      # data_monthly_new <- bind_rows(data_monthly_stlplus, data_monthly_new) 
      
      decomp_plot <- ggts_decomp(data_monthly_stlplus)
      print(decomp_plot + 
              labs(title =  paste("Seasonal Decomposition by Loess - ", i),
                   subtitle = paste("by stlplus(); Data = Trend + Seasonal + Remainder; ",
                                    "Horiz. Lines: Mean values"),
                   y = y_label_measure[i]))
    } 
    ## 'count' now NAs are replaced by values of Interpolated !!  #################
    # data_monthly <- data_monthly_new %>% 
    #   mutate(Year_Month = yearmonth(Year_Month),  # lost yearmonth() class by bind_row
    #          City = city) %>%  # lost yearmonth() by bind_rows
    #   as_tsibble(index = Year_Month, key = key)
#  }  
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```



```{r decomp with fabletools, eval = FALSE, fig.width= 7, fig.height= 10}
## decomposition - needs selected keys
# *Seasonal decomposition of time series by Loess (STL)*
  
data_monthly %>% 
  # filter(year(Year_Month) >=2000) %>% 
  model(STL(count ~ trend(window=trend_window) + season(window=season_window))) %>% 
  components() %>%
  autoplot() + theme(legend.position = "bottom") + xlab("Year")
  # facet_grid(vars(!!!key(data_monthly)), scales = "free")


```




## Periodicities - Season Frequency  

### Lag Plot - Differences 

```{r lag with single filter, eval = TRUE, fig.width= 7, fig.height= 5}

## !! for data with more than one time series
## => filter a single time series to use `gg_lag()` or `gg_tsdisplay()`
# 
# print(n_keys(data_monthly))   # number of separate time series
# print(key_data(data_monthly)) # tibble with key name(s) and subnames
# #     Measure       .rows        
# #   <chr>         <list>       
# # 1 Precipitation <int [1,596]>
# # 2 Temperature   <int [1,596]>
# print(key(data_monthly))  # provides "group" key name;
#                              for facet_wrap use: vars(!!!key(data_monthly))

year_filter <- 2000

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
  #  for (j in unique(key_data(data_monthly)[[2]])) {   # Measure
      
      plot_lag <- filter(data_monthly, year(Year_Month) >= year_filter & 
                           Measure == i) %>%   # City == i &Measure == j
        gg_lag(count, lags = 1:12, geom = "path", arrow = TRUE) +
        labs(x = y_label_measure[i], y = "lag(x, n)",
             col = "Month",
             title ="Lag by n months - y(t) plotted against y(t-n)",
             subtitle = paste(i, " - as of year", year_filter)) 
print(plot_lag)
    }
#  }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```

### ACF / PACF Correlogram

```{r ACF PACF plots, eval = FALSE}
# Plots covered by next chunk - Triplet tsdisplay plot

## ACF Correlogram
plot_1 <- data_monthly %>% ACF(count, lag_max = 24) %>% 
  autoplot() + 
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free", 
             strip.position = "left") +
  ggtitle("ACF Correlogram - w/ slow decrease as the lags increase", 
          subtitle = "- due to the trend, while the 'scalloped' shape is due the seasonality") +
  labs(y = paste("ACF ", "bounds = 1.96/sqrt(nrow(data)) = ",
                 round(2/sqrt(nrow(data)), digits = 3)))

## PACF - Partial ACF
plot_2 <- data_monthly %>% PACF(count, lag_max = 24) %>% 
  autoplot() +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left") +
  ggtitle("PACF - Partial Autocorrelation Function")

gridExtra::grid.arrange(plot_1, plot_2)
```


### Periodogram - Spectral Density Estimation of a Time Series

The spectral density characterizes the frequency content of the signal.
One purpose of estimating the spectral density is to detect any periodicities 
in the data, by observing peaks at the frequencies corresponding to these periodicities.

At frequency $\lambda = 1/12$ there is a significant peak
=> This pattern repeats every full frequency = every 12 months / every year

The remaining peaks are random and therefore cannot be assigned significantly.

```{r Triplet tsdisplay plot, eval = TRUE, fig.width= 7, fig.height= 5, error = TRUE}
## !! for data with more than one time series
## => filter a single time series to use `gg_lag()` or `gg_tsdisplay()`

## arttention: for Cottbus Plot with Spectrum throws an error =>error = TRUE !!
## - up to know only for Cottbus Precipitation (w/ Temperature running) 
## - Fehler in .$spec[, 1] : falsche Anzahl von Dimensionen

year_filter <- 2000

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) {
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {
   # for (j in unique(key_data(data_monthly)[[2]])) {
      plot_ts_spec <- data_monthly %>% 
        filter(year(Year_Month) >= year_filter & Measure == i) %>%  
        gg_tsdisplay(count,  lag_max = 24) +  # plot_type = "spectrum",
        labs( x = "Year", y = y_label_measure[i], 
              title = "Time series plot with ACF and Spectrum",
              subtitle = paste(i, " - as of year", year_filter))
      print(plot_ts_spec)
      
      plot_ts_pacf <- data_monthly %>% 
        filter(year(Year_Month) >= year_filter & Measure == i) %>%  
        gg_tsdisplay(count, plot_type = "partial", lag_max = 24) + 
        labs( x = "Year", y = y_label_measure[i], 
              title = "Time series plot with ACF and PACF",
              subtitle = paste(i, " - as of year", year_filter))
      print(plot_ts_pacf)
    }
#  } 
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```


### Seasonal vs non Seasonal ACF / Strength (Seasonal/Trend)

* Check acf1 and season_acf1 and compare with ACF Correlogram Plot
* acf1: first autocorrelation coefficient from the original data
* acf10: sum of square of the first ten autocorrelation coefficients from the original data
* diff1_acf1: first autocorrelation coefficient from the differenced data
* season_acf1: autocorrelation coefficient at the first seasonal lag

* Check Trend & Seasonal Strength close to 0 / 1 : weak / strong and compare them
* stl_e_acf1: first autocorrelation coefficient of the remainder series
* stl_e_acf10: sum of squares of the first ten autocorrelation coefficients of the remainder series
* linearity: linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component
* curvature: curvature of the trend component of the STL decomposition. It is based on the coefficient from an orthogonal quadratic regression applied to the trend component.


```{r seasonal vs non seasonal, eval = TRUE}

print("Check acf1 and season_acf1 and compare with ACF Correlogram Plot")
data_monthly %>% features(count, feat_acf)

print("Check Trend & Seasonal Strength close to 0 / 1 : weak / strong and compare them")
data_monthly %>% features(count, feat_stl)

plot_1 <- data_monthly %>% features(count, feat_acf) %>%
  ggplot(aes(x=acf1, y=season_acf1, col=Measure)) +
  geom_point() + facet_wrap(vars(Measure)) +
  theme(legend.position = "none") +
  ggtitle("Seasonal ACF lag1 vs. ACF lag1")
plot_2 <- data_monthly %>% features(count, feat_stl) %>%
  ggplot(aes(x=trend_strength, y=seasonal_strength_year, col=Measure)) +
  geom_point() + facet_wrap(vars(Measure)) +
  theme(legend.position = "none") +
  ggtitle("Seasonal vs. Trend Strength")
gridExtra::grid.arrange(plot_1, plot_2)


```


### Spectral Entropy Test

* Entropy close to 0 => series has strong trend and seasonality (=> easy to forecast)
* Entropy close to 1 => series is very noisy (and so is difficult to forecast) 

```{r Entropy test, eval = TRUE}
# (Shannon) spectral entropy of a time series, which is a measure of how easy 
# the series is to forecast. 
# A series which has strong trend and seasonality (and so is easy to forecast) 
# will have entropy close to 0. 
# A series that is very noisy (and so is difficult to forecast) 
# will have entropy close to 1.

print("Check entropy close to 0 or 1")
data_monthly %>% features(count, feat_spectral) 
# entropy close to 0 => series has strong trend and seasonality (=> easy to forecast)
# entropy close to 1 => series is very noisy (and so is difficult to forecast)

```


## Stationary Process Test

Strict-sense stationarity / Weak (wide-sense) stationarity

Augmented Dickey-Fuller test => type3, a linear model with both drift and linear trend

Trend Stationary - underlying trend (function solely of time) can be removed, 
leaving a stationary process

```{r eval = FALSE}
## Stationarity Test
library(aTSA)     # for stationary.test(data_ts) and adf.test(data_ts) - same
library(fractal)  # for stationaryty()
library(LaplacesDemon)

first_year<- data_monthly %$% year(min((Year_Month)))
# first_year <- as.integer(as_tibble(data_monthly) %>% 
#                            summarise(first_year = min(Year)))
freq <- 12
# data <- data_monthly_stlplus  # w/ numeric "Time"
# as.numeric(as.Date("2018-09-24") - as.Date("2017-10-24"))
# update data_ts with replaced NAs, e.g. adf.test() does not allow NAs
data_ts <- ts(data_monthly$count, start=c(first_year, 1), frequency = freq)

# Performs stationary test for a univariate time series
# combines the existing functions adf.test, pp.test and kpss.test 
# for testing the stationarity of a univariate time series x.
# null hypothesis of a unit root of a univarate time series x 
# (equivalently, x is a non-stationary time series)
# => p > 0.05 h0 can not be rejected  => non-stationary
# cat("Null Hypothesis of non-stationary time series - for p < 0.05: reject H_0")
# stationary.test(data_ts) 

# already covered by unitroot_kpss test
# data_monthly %>% 
#   features(count, list(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))

# The results are the same as one of the adf.test, pp.test, kpss.test, depending on which test 
# for MAuna Loa: adf.test: 

# adf.test(data_ts)  # Augmented Dickey-Fuller test

# kpss.test(data_ts) # KPSS Unit Root Test 
# pp.test(data_ts)   # Phillips-Perron Unit Root Test

# Priestley-Subba Rao (PSR) test
statio <-
  stationarity(data_monthly$count, n.taper = 6,
               n.block = max(c(12, floor(logb(length(data_ts), base = 12)))),
               significance = 0.05, center = TRUE, recenter = FALSE)
summary(statio)

is.stationary(data_monthly$count)
# statcheck() from Schlittgen
source("./Data_Schlittgen/tsutil.R")
statcheck(data_monthly$count, 5) #Berechnung der deskriptiven Maßzahlen ist elementar. 
     # Um die Kovarianzstationarität zu überprüfen, wird die Funktion eingesetzt
     #  plot ACF over Lag  for 5 segments
#  function statcheck determines the means, standard deviations and acf’s
#  of segments of a time series and plots the acf’s for the segments.

# already covered by unitroot_ndiffs, unitroot_nsdiffs tests
# data_monthly %>% 
#   features(count, list(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))
# vartable(data_ts,12) # variate Differenzen weisen darauf hin, dass einmal 
#                      # einfache und einmal saisonale Differenzen zu bilden sind
#                      # sind, um Stationarität zu erreichen.



# bandfilt(data_ts, 7, 12, 24)
# View(data_ts)
# 
# (model_lm_1 <- lm(count ~ Year_Month, data))
# summary(model_lm_1)
# plot(model_lm_1)
# 
# model_lm_2 <- lm(count ~ Year_Month + Year_Month^2, data)
# summary(model_lm_2)
# plot(model_lm_2)


```


```{r Beispiel 10.10, eval = FALSE}
library(dlm)                                                                                 
# co2 <- scan("./Data_Schlittgen/schauinsland.dat")  
co2 <- data_ts  # co2 <- ts(co2,start = 2005,frequency=12)
dlmco <- dlmModPoly(2) + dlmModSeas(12)
m1 <- c(382,0.1,rep(0,11))  
c1 <- diag(c( 0.1, 0.1, rep(100,11)))

buildFun <- function(x) { 
  W(dlmco)[1:3,1:3] <- diag(exp(x[1:3]))
  V(dlmco) <- exp(x[4]) 
  C0(dlmco) <- c1
  m0(dlmco) <- m1 
  return(dlmco)
}              

# Parameter estimation by maximum likelihood - very long runnning !!
fit <- dlmMLE(co2, parm=rep(1,4), build=buildFun)
fit$conv
dlm.co2 <- buildFun(fit$par)                                              
coFilter <- dlmFilter(co2, mod=dlm.co2)
coSmooth <- dlmSmooth(coFilter) 

plot(co2,type="o") 
lines(dropFirst(coFilter$m[,1]))
lines(dropFirst(coSmooth$s[,1]))
plot(dropFirst(coSmooth$s[,3]))

fut1 <- dlmForecast(coFilter, n=12)
x <- cbind(co2,fut1$f,fut1$f-1.96*sqrt(unlist(fut1$Q)),
           fut1$f+1.96*sqrt(unlist(fut1$Q)))                                
plot.ts(x,plot.type="s",lty=c(1,1,2,2)) 
points(2005+28/12,co2[29])




```


# Backup

## `r title_rmd` - Data Source 

**National Oceanic & Atmospheric Administration - Earth System Research Laboratory**

*NOAA ESRL *<https://www.esrl.noaa.gov/gmd/ccgg/trends/global.html>

Data file: Mauna Loa CO2 monthly mean data

<https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html>


## `r topic` - Average Yearly and Seasonal Data

```{r data print, eval = TRUE}

# caption_table <- y_label
caption_table <- "Temperature / degree Celsius rsp. Precipitation / mm/Month"
# print all Data or only slices of them
# knitr::kable(data_yearly, digits = 1,
#              caption = paste("Yearly", caption_table))

knitr::kable(slice(data_yearly, 1:11 ), digits = 1,
             caption = paste("Yearly", caption_table))
knitr::kable(slice(data_yearly, n() -10:0), digits = 1)


knitr::kable(mean_monthly, digits = 1,
             caption = paste("Monthly Means over all Years", caption_table))


```


## Test Plots

```{r ACF with ggplot, eval = FALSE, fig.width= 7, fig.height= 4}

# acf() does not allow NAs and facetting
data_ts <- ts(data_monthly$count, start=c(first_year, 1), frequency = freq)

# Korrelogramm
# „empirische Autokovarianz- und Autokorrelationsfunktion (ACF)
# 
#  Lag - Verschiebung, des betrachteten Zeitfensters
#  Lag 0: Jan mit Jan (sich selbst)
#  Lag 1: Jan mit Feb (next Feb , einen Monat weiter)
#  
#  Lag 12: Jan mit Jan (des nächsten Jahres)
#  t <- c(1,2,3) => lag(t): [1] NA  1  2
lag_max <- 4 * freq
data_acf <- acf(data_ts, lag.max = lag_max, plot = FALSE, type = "correlation")
data_pacf <- acf(data_ts, lag.max = lag_max, plot = FALSE, type = "partial")
# plot(data_acf)
 
# Temperaturzeitreihe weist natürliche saisonale Schwankungen auf
# => gut im Korellogramm zu erkennen kann
# Der Januar wird als Basis gewählt und die Monate um den Januar herum weisen 
#   eine sehr hohe positive Korrelation auf
# die Sommermonate in der Mitte (0,5) des Jahres weisen eine starke
#                  negative Korrelation auf
data_acf_tbl <- tibble(ACF = c(NA, as.vector(data_acf$acf[-1])), 
    # take out redundant first value lag = 0 => always one (also taken out by ACF()) !
                       PACF = c(NA, as.vector(data_pacf$acf)), # has no lag=0 value !
                       Lag = as.vector(freq*data_acf$lag))
data_acf_tbl %<>% pivot_longer(
  cols = c("ACF", "PACF"),
  names_to = "ACF_PACF",
  values_to = "value") 
bounds <- round(2/sqrt(data_acf$n.used), digits = 3)  # data_acf$n.used = nrow(data)
ggplot(data_acf_tbl, aes(Lag, value)) +
  geom_bar(stat = "identity", width = 0.2) +  
  # geom_line() +
  geom_hline(yintercept = bounds, col = "blue", lty = "dashed") +
  geom_hline(yintercept = -bounds, col = "blue", lty = "dashed") +  
  facet_wrap( ~ ACF_PACF, ncol = 1, scales = "free", strip.position = "left") +
  ggtitle(paste("Time Series Correlogram"),
          subtitle = paste("with Season Frequency = ", freq, ",  lag.max = ", lag_max)) +
  labs(y = "", x = "Lag / months")

  

```


```{r eval = FALSE, fig.width= 7, fig.height= 4}
# Periodogramm
# Das Periodogramm ist das Pendant zum Korellogramm auf Spektralebene
# Es ist die abgebildete Fouriertransformation der Autokovarianzfunktion 
data_period <- 
  spec.pgram(data_ts, taper = 0, pad = 0, fast = TRUE, demean = FALSE,
             detrend = TRUE, plot = FALSE, na.action = na.fail)
#  Frequenz λ = 1: signiﬁkanter Ausschlag stattﬁndet
#  => vorliegende Muster wiederholt sich jede volle Frequenz / jedes Jahr
#  Die restlichen Ausschläge sind bereits zu zufällig, um sie signiﬁkant 
#                                                    einordnen zu können.

data_period_tbl <- as_tibble(data_period$freq) %>% 
  mutate(Frequency = value,
         Spectrum = data_period$spec)

ggplot(data_period_tbl, aes(Frequency, Spectrum)) +
  geom_line() +
  # geom_point() +
  labs(x = paste("Frequency\n", "bandwidth = ", data_period$bandwidth)) +
  scale_y_log10() +
  ggtitle( paste("Time Series", data_period$method))

```



```{r yearly data with replaced NAs, eval = FALSE}

### now with replaced NAs

data_monthly_wide <- data_monthly %>%
  pivot_wider(id_cols = c(Year, Measure),
              names_from = Month,
              values_from = count) %>% 
  mutate(City = city)

# data_yearly - generation from data_monthly_wide
#     data_check_and_fill_w_na(data_monthly,
# w/o gaps and as tsibble (no winter, ... since season = FALSE), maybe w/ NAs
data_yearly <- data_monthly_wide %>% 
  # group_by(City, Measure) %>% 
  # grouping important, otherwise winter rolling over Measure
  uts_gen_yearly_seasonal_avg(season = TRUE) %>% 
  dplyr::select(-(Jan:Dec))
# fill_gaps with NAs since (since ts objects don't allow missing years)
data_yearly <- as_tsibble(data_yearly, index = Year, key = key) 

nr_series_w_gaps <- ifelse(has_gaps(data_yearly, .full = TRUE)$.gaps, 1, 0) 

if (sum(nr_series_w_gaps) > 0) {    # Check for missing years
  print(data_yearly %>% scan_gaps(.full = TRUE))  # Reveal
  print(data_yearly %>% count_gaps(.full = TRUE)) # Summarise - number of gaps from - to
  data_yearly %<>% fill_gaps(.full = TRUE)       # Fill in time gaps
  
  print(data_yearly %>% has_gaps(.full = TRUE))   # Check for gaps
}
  
```

```{r end, echo = FALSE}
###########################################################################
Prog.End <- Sys.time()
run.time <- round((Prog.End - Prog.Start), digits = 2)
message("Program executed on: ", Prog.End, "\t Program run time: ", run.time, " secs")
```
