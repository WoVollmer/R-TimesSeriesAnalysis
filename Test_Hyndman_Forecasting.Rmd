---
title: "Forecasting: Principles and Practice -"
subtitle: "Rob J Hyndman and George Athanasopoulos"
author: "Wolfgang Vollmer"
date: '`r Sys.Date()`'
output:
  pdf_document:
    fig_caption: no
    fig_height: 6
    fig_width: 12
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document: default
  word_document:
    toc: yes
  odt_document:
urlcolor: blue
papersize: a4
params:
  city: "Basel"
  start: !r Sys.time()
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
)
```



```{r initialization, include = FALSE}
# rm(list=ls()) # deletes all existing objects / variables !!

library(tidyverse)

suppressMessages(library(magrittr))
library(lubridate)
library(fpp3)  # tsibble, tsibbledata, fable, and feasts packages & some tidyverse
library(stlplus) # Enhanced Seasonal Decomposition of Time Series by Loess
library(DT)     # R interface to the JavaScript library DataTables

library(GGally)  # matrix scatterplots
library(broom)
library(rlang)

city <- params$city
params$start

```

# Getting Started

```{r Getting Started}



```

# Time Series Graphics

## Tsibble Objects

```{r Time Series Graphics}

y <- tsibble(Year = 2015:2019, Observation = c(123,39,78,52,110), index = Year)

z <- tibble(Month = c("2019 Jan", "2019 Feb", "2019 Mar", "2019 Apr", 
                      "2019 May"), Observation = c(50, 23, 34, 30, 25))
z %>%
  mutate(Month = yearmonth(Month)) %>%
  as_tsibble(index = Month)

olympic_running

PBS

PBS %>%
  filter(ATC2=="A10") %>%
  select(Month, Concession, Type, Cost)

PBS %>%
  filter(ATC2=="A10") %>%
  select(Month, Concession, Type, Cost) %>%
  summarise(TotalC = sum(Cost)) # rsp. summarise(TotalC = mean(Cost))

PBS %>%
  filter(ATC2=="A10" & Month == yearmonth("1992 Mar"))

PBS %>%
  filter(ATC2=="A10") %>%
  select(Month, Concession, Type, Cost) %>%
  summarise(TotalC = sum(Cost)) %>%
  mutate(Cost = TotalC/1e6)

PBS %>%
  filter(ATC2=="A10") %>%
  select(Month, Concession, Type, Cost) %>%
  summarise(TotalC = sum(Cost)) %>%
  mutate(Cost = TotalC/1e6) -> a10

prison <- readr::read_csv("https://OTexts.com/fpp3/extrafiles/prison_population.csv")
prison <- prison %>%
  mutate(quarter = yearquarter(date)) %>%
  select(-date) %>%
  as_tsibble(key = c(state, gender, legal, indigenous), index = quarter)

prison

```

## Time Plots

```{r Time Plots}
melsyd_economy <- ansett %>%
  filter(Airports == "MEL-SYD", Class=="Economy")
melsyd_economy %>%
  autoplot(Passengers) +
    labs(title = "Ansett economy class passengers", subtitle = "Melbourne-Sydney") +
    xlab("Year")

a10 %>% autoplot(Cost) +
  ggtitle("Antidiabetic drug sales") +
  ylab("$ million") + xlab("Year")

```

## Seasonal Plots

```{r Seasonal Plots}
a10 %>% gg_season(Cost, labels = "both") +
  ylab("$ million") +
  ggtitle("Seasonal plot: antidiabetic drug sales")

vic_elec %>% gg_season(Demand, period="day") + theme(legend.position = "none")

vic_elec %>% gg_season(Demand, period="week") + theme(legend.position = "none")

vic_elec %>% gg_season(Demand, period="year")

```


## Seasonal Subseries Plots

```{r Seasonal Subseries Plots}
a10 %>%
  gg_subseries(Cost) +
    ylab("$ million") +
    xlab("Year") +
    ggtitle("Seasonal subseries plot: antidiabetic drug sales")

holidays <- tourism %>%
  filter(Purpose == "Holiday") %>%
  group_by(State) %>%
  summarise(Trips = sum(Trips))

holidays

holidays %>% autoplot(Trips) +
  ylab("thousands of trips") + xlab("Year") +
  ggtitle("Australian domestic holiday nights")

holidays %>% gg_season(Trips) +
  ylab("thousands of trips") +
  ggtitle("Australian domestic holiday nights")

holidays %>%
  gg_subseries(Trips) + ylab("thousands of trips") +
  ggtitle("Australian domestic holiday nights")

```

## Scatter Plots

```{r Scatter Plots}

vic_elec %>%
  filter(year(Time) == 2014) %>%
  autoplot(Demand) +
    xlab("Year: 2014") + ylab(NULL) +
    ggtitle("Half-hourly electricity demand: Victoria, Australia")

vic_elec %>%
  filter(year(Time) == 2014) %>%
  autoplot(Temperature) +
    xlab("Year: 2014") + ylab(NULL) +
    ggtitle("Half-hourly temperatures: Melbourne, Australia")

vic_elec %>%
  filter(year(Time) == 2014) %>%
  ggplot(aes(x = Temperature, y = Demand)) +
    geom_point() +
    ylab("Demand (GW)") + xlab("Temperature (Celsius)")


visitors <- tourism %>%
  group_by(State) %>%
  summarise(Trips = sum(Trips))
visitors %>%
  ggplot(aes(x = Quarter, y = Trips)) +
    geom_line() +
    facet_grid(vars(State), scales = "free_y") +
    ylab("Number of visitor nights each quarter (millions)")

visitors %>%
  spread(State, Trips) %>%
  GGally::ggpairs(columns = 2:9)


```


## Lag Plots

```{r Lag Plots}

recent_production <- aus_production %>%
  filter(year(Quarter) >= 1992)
recent_production %>% gg_lag(Beer, geom="point")

```

## Autocorrelation & White Noise

```{r Autocorrelation}

recent_production %>% ACF(Beer, lag_max = 9)

recent_production %>% ACF(Beer) %>% autoplot()

a10 %>% ACF(Cost, lag_max = 48) %>% autoplot()

set.seed(30)
y <- tsibble(sample = 1:50, wn = rnorm(50), index = sample)
y %>% autoplot(wn) + ggtitle("White noise")

y %>% ACF(wn) %>% autoplot()

```

## Exercises Google closing stock prices

```{r Exercises 2.10 Google}
dgoog <- gafa_stock %>%
  filter(Symbol == "GOOG", year(Date) >= 2018) %>%
  mutate(trading_day = row_number()) %>%
  update_tsibble(index = trading_day, regular = TRUE) %>%
  mutate(diff = difference(Close))
```


# Time Series Decomposition

## Transformations and Adjustments

```{r Time Series Decomposition}

global_economy %>%
  filter(Country == "Australia") %>%
  autoplot(GDP / Population)

print_retail <- aus_retail %>%
  filter(Industry == "Newspaper and book retailing") %>%
  group_by(Industry) %>%
  index_by(Year = year(Month)) %>%
  summarise(Turnover = sum(Turnover))
aus_economy <- global_economy %>%
  filter(Code == "AUS")
print_retail %>%
  left_join(aus_economy, by = "Year") %>%
  mutate(Adjusted_turnover = Turnover / CPI) %>%
  gather("Type", "Turnover", Turnover, Adjusted_turnover, factor_key = TRUE) %>%
  ggplot(aes(x = Year, y = Turnover)) +
    geom_line() +
    facet_grid(vars(Type), scales = "free_y") +
    xlab("Years") + ylab(NULL) +
    ggtitle("Turnover for the Australian print media industry")

lambda <- aus_production %>%
  features(Gas, features = guerrero) %>%
  pull(lambda_guerrero)
aus_production %>% autoplot(box_cox(Gas, lambda))

```

## Time Series Components

```{r Time Series Components, fig.width= 7, fig.height= 4}

us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade") %>%
  select(-Series_ID)
us_retail_employment %>%
  autoplot(Employed) +
  xlab("Year") + ylab("Persons (thousands)") +
  ggtitle("Total employment in US retail")

dcmp <- us_retail_employment %>%
  model(STL(Employed))
components(dcmp)

us_retail_employment %>%
  autoplot(Employed, color='gray') +
  autolayer(components(dcmp), trend, color='red') +
  xlab("Year") + ylab("Persons (thousands)") +
  ggtitle("Total employment in US retail")

components(dcmp) %>% autoplot() + xlab("Year")

us_retail_employment %>%
  autoplot(Employed, color='gray') +
  autolayer(components(dcmp), season_adjust, color='blue') +
  autolayer(components(dcmp), trend, color='red') +
  xlab("Year") + ylab("Persons (thousands)") +
  ggtitle("Total employment in US retail")

```

## Moving averages

```{r Moving averages, fig.width= 7, fig.height= 4}

global_economy %>%
  filter(Country == "Australia") %>%
  autoplot(Exports) +
  xlab("Year") + ylab("% of GDP") +
  ggtitle("Total Australian exports")

aus_exports <- global_economy %>%   # slide_db => rolling mean
  filter(Country == "Australia") %>%
  mutate(
    `5-MA` = slide_dbl(Exports, mean, .size = 5, .align = "center")
  )

aus_exports <- global_economy %>%  # slide_db => rolling mean, same result
  filter(Country == "Australia") %>%
  mutate(
    `5-MA` = slide_dbl(Exports,  ~ mean(.), .size = 5),
    stretch = stretch_dbl(Exports,  ~ mean(.), .init = 5) # up to mean overall
  )

aus_exports_tile <- global_economy %>%  # tile_db => slot by slot => only few data
  filter(Country == "Australia") %$%
  tile_dbl(Exports,  ~ mean(.), .size = 5)

aus_exports %>%
  autoplot(Exports) +
  autolayer(aus_exports, `5-MA`, color='red') +
  autolayer(aus_exports, stretch, color='blue') +
  xlab("Year") + ylab("Exports (% of GDP)") +
  ggtitle("Total Australian exports") +
  guides(colour=guide_legend(title="series"))


beer <- aus_production %>%
  filter(year(Quarter) >= 1992) %>%
  select(Quarter, Beer)
beer_ma <- beer %>%
  mutate(
    `4-MA` = slide_dbl(Beer, mean, .size = 4, .align = "center-left"),
    `2x4-MA` = slide_dbl(`4-MA`, mean, .size = 2, .align = "center-right")
  )

us_retail_employment_ma <- us_retail_employment %>%
  mutate(
    `12-MA` = slide_dbl(Employed, mean, .size = 12, .align = "cr"),
    `2x12-MA` = slide_dbl(`12-MA`, mean, .size = 2, .align = "cl")
  )
us_retail_employment_ma %>%
  autoplot(Employed, color='gray') +
  autolayer(us_retail_employment_ma, vars(`2x12-MA`), color='red') +
  xlab("Year") + ylab("Persons (thousands)") +
  ggtitle("Total employment in US retail")
```

## Classical and STL decomposition

```{r Classical decomposition, fig.width= 7, fig.height= 4}

us_retail_employment %>%
  model(classical_decomposition(Employed, type = "additive")) %>%
  components() %>%
  autoplot() + xlab("Year") +
  ggtitle("Classical additive decomposition of total US retail employment")

us_retail_employment %>%
  model(STL(Employed ~ trend(window=7) + season(window='periodic'),
    robust = TRUE)) %>%
  components() %>%
  autoplot()

```


# Time Series Features

## Simple statistics & ACF & STL

```{r Simple statistics ACF STL, fig.width= 7, fig.height= 4}

# numerical summary computed from a time series is a feature of that time series 

tourism %>% features(Trips, mean)  # V1 contains the mean of each time series.

tourism %>% features(Trips, list(mean=mean)) %>% arrange(mean) # mit Benamsung

tourism %>% features(Trips, quantile, prob=seq(0,1,by=0.25))

tourism %>% features(Trips, feat_acf)

tourism %>%
  features(Trips, feat_stl)

tourism %>%
  features(Trips, feat_stl) %>%
  ggplot(aes(x=trend_strength, y=seasonal_strength_year, col=Purpose)) +
    geom_point() + facet_wrap(vars(State))

tourism %>%
  features(Trips, feat_stl) %>%
  filter(seasonal_strength_year == max(seasonal_strength_year)) %>%
  left_join(tourism, by = c("State","Region","Purpose")) %>%
  ggplot(aes(x = Quarter, y = Trips)) + geom_line() +
    facet_grid(vars(State,Region,Purpose))

# data_monthly %>% features(count, mean)
# # A tibble: 2 x 2
#   Temp_Precip      V1
#   <chr>         <dbl>
# 1 Precipitation 66.0 
# 2 Temperature    9.50
# 
# data_monthly %>% features(count, feat_acf)
# data_monthly %>% features(count, feat_stl)
# 
# data_monthly %>% features(count, feat_stl) %>%
#   ggplot(aes(x=trend_strength, y=seasonal_strength_year, col=Temp_Precip)) +
#     geom_point() + facet_wrap(vars(Temp_Precip))


```

## Exploring Australian tourism data

```{r Exploring Australian tourism data, fig.width= 7, fig.height= 4}

tourism_features <- tourism %>%
  features(Trips, feature_set(pkgs="feasts"))
tourism_features

tourism_features %>%
  select_at(vars(contains("season"), Purpose)) %>%
  mutate(
    seasonal_peak_year = glue::glue("Q{seasonal_peak_year+1}"),
    seasonal_trough_year = glue::glue("Q{seasonal_trough_year+1}"),
  ) %>%
  GGally::ggpairs(mapping = aes(colour=Purpose))

library(broom)
pcs <- tourism_features %>%
  select(-State, -Region, -Purpose) %>%
  prcomp(scale=TRUE) %>%
  augment(tourism_features)
pcs %>%
  ggplot(aes(x=.fittedPC1, y=.fittedPC2, col=Purpose)) +
  geom_point() + theme(aspect.ratio=1)

outliers <- pcs %>%
  filter(.fittedPC1 > 10.5) %>%
  select(Region, State, Purpose, .fittedPC1, .fittedPC2)
outliers

outliers %>%
  left_join(tourism, by = c("State", "Region", "Purpose")) %>%
  mutate(Series = glue::glue("{State}", "{Region}", "{Purpose}", .sep = "\n\n")) %>%
  ggplot(aes(x = Quarter, y = Trips)) +
    geom_line() +
    facet_grid(Series ~ ., scales='free') +
    ggtitle("Outlying time series in PC space")


```


# The Forecaster's Toolbox

## A Tidy Forecasting Workflow

```{r Tidy Forecasting Workflow, fig.width= 7, fig.height= 4}
global_economy %>%
  filter(Country=="Sweden") %>%
  autoplot(GDP) +
    ggtitle("GDP for Sweden") + ylab("$US billions")

TSLM(GDP ~ trend())

fit <- global_economy %>%
  model(trend_model = TSLM(GDP ~ trend()))
fit

fit %>% forecast(h = "3 years")

fit %>% forecast(h = "3 years") %>%
  filter(Country=="Sweden") %>%
  autoplot(global_economy) +
    ggtitle("GDP for Sweden") + ylab("$US billions")
```

## Some simple forecasting methods

```{r simple forecasting methods, fig.width= 7, fig.height= 4}

bricks <- aus_production %>% filter_index(1970 ~ 2004)

bricks %>% model(MEAN(Bricks))

bricks %>% model(NAIVE(Bricks))
# RW(Bricks) # Random Walk Model: is an equivalent alternative

bricks %>% model(SNAIVE(Bricks ~ lag("year")))

bricks %>% model(RW(Bricks ~ drift()))

# Set training data from 1992 to 2006
train <- aus_production %>% filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train %>%
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
# Generate forecasts for 14 quarters
beer_fc <- beer_fit %>% forecast(h=14)
# Plot forecasts against actual values
beer_fc %>%
  autoplot(train, level = NULL) +
    autolayer(filter_index(aus_production, "2007 Q1" ~ .), color = "black") +
    ggtitle("Forecasts for quarterly beer production") +
    xlab("Year") + ylab("Megalitres") +
    guides(colour=guide_legend(title="Forecast"))

# Re-index based on trading days
google_stock <- gafa_stock %>%
  filter(Symbol == "GOOG") %>%
  mutate(day = row_number()) %>%
  update_tsibble(index = day, regular = TRUE)
# Filter the year of interest
google_2015 <- google_stock %>% filter(year(Date) == 2015)
# Fit the models
google_fit <- google_2015 %>%
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = NAIVE(Close ~ drift())
  )
# Produce forecasts for the 19 trading days in January 2015
google_fc <- google_fit %>% forecast(h = 19)
# A better way using a tsibble to determine the forecast horizons
google_jan_2016 <- google_stock %>%
  filter(yearmonth(Date) == yearmonth("2016 Jan"))
google_fc <- google_fit %>% forecast(google_jan_2016)
# Plot the forecasts
google_fc %>%
  autoplot(google_2015, level = NULL) +
    autolayer(google_jan_2016, Close, color='black') +
    ggtitle("Google stock (daily ending 31 Dec 2015)") +
    xlab("Day") + ylab("Closing Price (US$)") +
    guides(colour=guide_legend(title="Forecast"))

```

## Residuals

```{r residuals}

augment(beer_fit)

google_2015 %>% autoplot(Close) +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google Stock in 2015")

aug <- google_2015 %>% model(NAIVE(Close)) %>% augment()
aug %>% autoplot(.resid) + xlab("Day") + ylab("") +
  ggtitle("Residuals from naïve method")

aug %>%
  ggplot(aes(x = .resid)) +
  geom_histogram() +
  ggtitle("Histogram of residuals")

aug %>% ACF(.resid) %>% autoplot() + ggtitle("ACF of residuals")

google_2015 %>% model(NAIVE(Close)) %>% gg_tsresiduals()

# lag=h and fitdf=K
aug %>% features(.resid, box_pierce, lag=10, dof=0)

aug %>% features(.resid, ljung_box, lag=10, dof=0)
```
 
 ## Prediction intervals

```{r Prediction intervals, fig.width= 7, fig.height= 4}

google_2015 %>%
  model(NAIVE(Close)) %>%
  forecast(h = 10) %>%
  hilo()

google_2015 %>%
  model(NAIVE(Close)) %>%
  forecast(h = 10) %>%
  autoplot(google_2015)

fit <- google_2015 %>%
  model(NAIVE(Close))
sim <- fit %>% generate(h = 30, times = 5, bootstrap = TRUE)
sim

google_2015 %>%
  ggplot(aes(x = day)) +
  geom_line(aes(y = Close)) +
  geom_line(aes(y = .sim, colour = as.factor(.rep)), data = sim) +
  ggtitle("Google closing stock price") +
  guides(col = FALSE)

fc <- fit %>% forecast(h = 30, bootstrap = TRUE)
fc

fc %>% autoplot(google_2015) +
  ggtitle("Google closing stock price")

google_2015 %>%
  model(NAIVE(Close)) %>%
  forecast(h = 10, bootstrap = TRUE, times = 1000) %>%
  hilo()

```

## Evaluating forecast accuracy

```{r Evaluating forecast accuracy, fig.width= 7, fig.height= 4}

# Functions to subset a time series

aus_production %>% filter(year(Quarter) >= 1995)

aus_production %>% filter(quarter(Quarter) == 1) # extracts all first quarters

aus_production %>% slice(n()-19:0)  # extracts the last 20 observations

aus_retail %>%
  group_by(State, Industry) %>%
  slice(1:12)

gafa_stock %>%
  group_by(Symbol) %>%
  top_n(1, Close)

# Forecast errors

recent_production <- aus_production %>% filter(year(Quarter) >= 1992)
beer_train <- recent_production %>% filter(year(Quarter) <= 2007)

beer_fit <- beer_train %>%
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())  # RW - Random Walk
  )

beer_fc <- beer_fit %>%
  forecast(h = 10)

beer_fc %>%
  autoplot(filter(aus_production, year(Quarter) >= 1992), level = NULL) +
  xlab("Year") + ylab("Megalitres") +
  ggtitle("Forecasts for quarterly beer production") +
  guides(colour=guide_legend(title="Forecast"))

google_fit <- google_2015 %>%
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = RW(Close ~ drift())
  )

google_fc <- google_fit %>%
  forecast(google_jan_2016)

google_fc %>%
  autoplot(rbind(google_2015,google_jan_2016), level = NULL) +
  xlab("Day") + ylab("Closing Price (US$)") +
  ggtitle("Google stock price (daily ending 6 Dec 13)") +
  guides(colour=guide_legend(title="Forecast"))

accuracy(google_fc, google_stock)

```

## Time seriescross-validation

```{r Time seriescross-validation, fig.width= 7, fig.height= 4}
# Time series cross-validation accuracy
google_2015_tr <- google_2015 %>%
  slice(1:(n()-1)) %>%
  stretch_tsibble(.init = 3, .step = 1)
fc <- google_2015_tr %>%
  model(RW(Close ~ drift())) %>%
  forecast(h=1)

fc %>% accuracy(google_2015)

# Residual accuracy
google_2015 %>% model(RW(Close ~ drift())) %>% accuracy()

google_2015_tr <- google_2015 %>%
  slice(1:(n()-8)) %>%
  stretch_tsibble(.init = 3, .step = 1)

fc <- google_2015_tr %>%
  model(RW(Close ~ drift())) %>%
  forecast(h=8) %>%
  group_by(.id) %>%
  mutate(h = row_number()) %>%
  ungroup()

fc %>%
  accuracy(google_2015, by = "h") %>%
  ggplot(aes(x = h, y = RMSE)) + geom_point()

```

##  Forecasting using transformations

```{r Forecasting using transformations, fig.width= 7, fig.height= 4}

scaled_logit <- new_transformation(
  transformation = function(x, lower=0, upper=1){
    log((x-lower)/(upper-x))
  },
  inverse = function(x, lower=0, upper=1){
    (upper-lower)*exp(x)/(1+exp(x)) + lower
  }
)

eggs <- as_tsibble(fma::eggs)
fit <- eggs %>% model(RW(log(value) ~ drift()))
fc <- fit %>% forecast(h=50) %>%
  mutate(Forecast = "Bias adjusted")
fc_biased <- fit %>% forecast(h=50, bias_adjust = FALSE) %>%
  mutate(Forecast = "Simple back transformation")
eggs %>% autoplot(value) +
  autolayer(fc_biased, level = 80) +
  autolayer(fc, colour = "red", level = NULL)

```


## Forecasting with decomposition

```{r Forecasting with decomposition, fig.width= 7, fig.height= 4}

us_retail_employment <- us_employment %>%
  filter(year(Month) >= 1990, Title == "Retail Trade")
dcmp <- us_retail_employment %>%
  model(STL(Employed ~ trend(window = 7), robust=TRUE)) %>%
  components() %>%
  select(-.model)
dcmp %>%
  model(NAIVE(season_adjust)) %>%
  forecast() %>%
  autoplot(dcmp) + ylab("New orders index") +
  ggtitle("Naive forecasts of seasonally adjusted data")

us_retail_employment %>%
  model(stlf = decomposition_model(
             STL(Employed ~ trend(window = 7), robust = TRUE),
             NAIVE(season_adjust)
  )) %>%
  forecast() %>%
  autoplot(us_retail_employment)

```



# Judgement Forecasts

```{r Judgement Forecasts, eval = FALSE, fig.width= 7, fig.height= 4}

# no R code

```


# Time Series Regression Models

## The linear model

```{r Time Series Regression Models,  eval = FALSE, fig.width= 7, fig.height= 4}

us_change %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Consumption, colour = "Consumption")) +
  geom_line(aes(y = Income, colour = "Income")) +
  ylab("% change") + xlab("Year") +
  guides(colour=guide_legend(title="series"))

us_change %>%
  ggplot(aes(x=Income, y=Consumption)) +
    ylab("Consumption (quarterly % change)") +
    xlab("Income (quarterly % change)") +
    geom_point() +
    geom_smooth(method="lm", se=FALSE)

us_change %>%
  model(TSLM(Consumption ~ Income)) %>%
  report()

us_change %>%
  GGally::ggpairs(columns = 2:6)

```

##Least squares estimation

```{r  Least squares estimation,  fig.width= 7, fig.height= 4}

fit.consMR <- us_change %>%
  model(
    tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings)
  )
report(fit.consMR)

augment(fit.consMR) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Consumption, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  xlab("Year") + ylab(NULL) +
  ggtitle("Percent change in US consumption expenditure") +
  guides(colour=guide_legend(title=NULL))

augment(fit.consMR) %>%
  ggplot(aes(x=Consumption, y=.fitted)) +
  geom_point() +
  ylab("Fitted (predicted values)") +
  xlab("Data (actual values)") +
  ggtitle("Percent change in US consumption expenditure") +
  geom_abline(intercept=0, slope=1)

```

## Evaluating the regression model

```{r Evaluating the regression model, fig.width= 7, fig.height= 4}

fit.consMR %>% gg_tsresiduals()

augment(fit.consMR) %>% features(.resid, ljung_box, lag = 10, dof = 5)

df <- left_join(us_change, residuals(fit.consMR), by = "Quarter")
p1 <- ggplot(df, aes(x=Income, y=.resid)) +
  geom_point() + ylab("Residuals")
p2 <- ggplot(df, aes(x=Production, y=.resid)) +
  geom_point() + ylab("Residuals")
p3 <- ggplot(df, aes(x=Savings, y=.resid)) +
  geom_point() + ylab("Residuals")
p4 <- ggplot(df, aes(x=Unemployment, y=.resid)) +
  geom_point() + ylab("Residuals")
# (p1 | p2) / (p3 | p4)
gridExtra::grid.arrange(p1, p2, p3, p4)

augment(fit.consMR) %>%
  ggplot(aes(x=.fitted, y=.resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")

fit <- aus_airpassengers %>%
  left_join(guinea_rice, by = "Year") %>%
  model(TSLM(Passengers ~ Production))
report(fit)

fit %>% gg_tsresiduals()

```

##  Some useful predictors & Selecting

```{r  Some useful predictors, fig.width= 7, fig.height= 4}

recent_production <- aus_production %>%
  filter(year(Quarter) >= 1992)
recent_production %>%
  autoplot(Beer) +
  labs(x = "Year", y = "Megalitres")

fit_beer <- recent_production %>%
  model(TSLM(Beer ~ trend() + season()))
report(fit_beer)

augment(fit_beer) %>%
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Beer, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(x = "Year", y = "Megalitres",
       title = "Quarterly Beer Production")

augment(fit_beer) %>%
  ggplot(aes(x = Beer, y = .fitted,
             colour = factor(quarter(Quarter)))) +
    geom_point() +
    ylab("Fitted") + xlab("Actual values") +
    ggtitle("Quarterly beer production") +
    scale_colour_brewer(palette="Dark2", name="Quarter") +
    geom_abline(intercept=0, slope=1)

fourier_beer <- recent_production %>%
  model(TSLM(Beer ~ trend() + fourier(K=2)))
report(fourier_beer)

glance(fit.consMR) %>% select(adj_r_squared, CV, AIC, AICc, BIC)

```

##  Forecasting with regression

```{r Forecasting with regression, fig.width= 7, fig.height= 4}

recent_production <- aus_production %>% filter(year(Quarter) >= 1992)
fit_beer <- recent_production %>%
  model(TSLM(Beer ~ trend() + season()))
fc_beer <- forecast(fit_beer)
fc_beer %>%
  autoplot(recent_production) +
  ggtitle("Forecasts of beer production using regression") +
  xlab("Year") + ylab("megalitres")

fit_consBest <- us_change %>%
  model(lm = TSLM(Consumption ~ Income + Savings + Unemployment))
up_future <- new_data(us_change, 4) %>%
  mutate(Income = 1, Savings = 0.5, Unemployment = 0)
down_future <- new_data(us_change, 4) %>%
  mutate(Income = -1, Savings = -0.5, Unemployment = 0)
fc_up <- forecast(fit_consBest, new_data = up_future) %>%
  mutate(Scenario = "Increase") %>%
  as_fable(response=Consumption, key = Scenario)
fc_down <- forecast(fit_consBest, new_data = down_future) %>%
  mutate(Scenario = "Decrease") %>%
  as_fable(response=Consumption, key = Scenario)

fit_cons <- us_change %>%
  model(TSLM(Consumption ~ Income))
new_cons <- new_data(us_change, n = 4) %>%
  mutate(Income = mean(us_change$Income), Scenario = "Average increase")
fcast_ave <- forecast(fit_cons, new_cons) %>%
  as_fable(response = Consumption, key = Scenario)
new_cons <- new_data(us_change, n = 4) %>%
  mutate(Income = 12, Scenario = "Extreme increase")
fcast_up <- forecast(fit_cons, new_cons) %>%
  as_fable(response = Consumption, key = Scenario)

us_change %>%
  autoplot(Consumption) +
  autolayer(rbind(fcast_ave, fcast_up)) +
  ylab("% change in US consumption")



```

##  Nonlinear regression

```{r  Nonlinear regression, fig.width= 7, fig.height= 4}

boston_men <- boston_marathon %>%
  filter(Event == "Men's open division") %>%
  mutate(Minutes = as.numeric(Time)/60)

fit_trends <- boston_men %>%
  model(
    linear = TSLM(Minutes ~ trend()),
    exponential = TSLM(log(Minutes) ~ trend()),
    piecewise = TSLM(Minutes ~ trend(knots = c(1940, 1980)))
  )
fc_trends <- fit_trends %>% forecast(h=10)

boston_men %>%
  autoplot(Minutes) +
  geom_line(aes(y=.fitted, colour=.model), data = fitted(fit_trends)) +
  autolayer(fc_trends, alpha = 0.5, level = 95) +
  xlab("Year") +  ylab("Winning times in minutes") +
  ggtitle("Boston Marathon") +
  guides(colour=guide_legend(title="Model"))

```



# Exponential Smoothing - ETS

## Simple exponential smoothing

```{r Simple Exponential Smoothing, fig.width= 7, fig.height= 4}

algeria_economy <- tsibbledata::global_economy %>%
  filter(Country == "Algeria")
algeria_economy %>%
  autoplot(Exports) +
  ylab("Exports (% of GDP)") + xlab("Year")

# Estimate parameters
fit <- algeria_economy %>%
  model(ETS(Exports ~ error("A") + trend("N") + season("N"), opt_crit = "mse"))
fc <- fit %>%
  forecast(h = 5)

fc %>%
  autoplot(algeria_economy) +
  geom_line(aes(y = .fitted, colour = "Fitted"), data = augment(fit)) +
  ylab("Exports (% of GDP)") + xlab("Year")


```

## Methods with trend

```{r Methods with trend, fig.width= 7, fig.height= 4}

aus_economy <- global_economy %>%
  filter(Code == "AUS") %>%
  mutate(Pop = Population / 1e6)
fit <- aus_economy %>%
  model(AAN = ETS(Pop ~ error("A") + trend("A") + season("N")))
fc <- fit %>% forecast(h = 10)

aus_economy %>%
  model(
    `Holt's method` = ETS(Pop ~ error("A") + trend("A") + season("N")),
    `Damped Holt's method` = ETS(Pop ~ error("A") + trend("Ad", phi = 0.9) + season("N"))
  ) %>%
  forecast(h = 15) %>%
  autoplot(aus_economy, level = NULL) +
  ggtitle("Forecasts from Holt's method") + xlab("Year") +
  ylab("Population of Australia (millions)") +
  guides(colour = guide_legend(title = "Forecast"))

www_usage <- as_tsibble(WWWusage)
www_usage %>% autoplot(value) +
  xlab("Minute") + ylab("Number of users")

www_usage %>%
  stretch_tsibble(.init = 10) %>%
  model(
    SES = ETS(value ~ error("A") + trend("N") + season("N")),
    Holt = ETS(value ~ error("A") + trend("A") + season("N")),
    Damped = ETS(value ~ error("A") + trend("Ad") + season("N"))
  ) %>%
  forecast(h = 1) %>%
  accuracy(www_usage)

fit <- www_usage %>%
  model(Damped = ETS(value ~ error("A") + trend("Ad") + season("N")))

tidy(fit)

fit %>%
  forecast(h = 10) %>%
  autoplot(www_usage) +
  xlab("Minute") + ylab("Number of users")

```

## Methods with seasonality

```{r Methods with seasonality, fig.width= 7, fig.height= 4}

aus_holidays <- tourism %>%
  filter(Purpose == "Holiday") %>%
  summarise(Trips = sum(Trips))
fit <- aus_holidays %>%
  model(
    additive = ETS(Trips ~ error("A") + trend("A") + season("A")),
    multiplicative = ETS(Trips ~ error("M") + trend("A") + season("M"))
  )
fc <- fit %>% forecast(h = "3 years")

fc %>%
  autoplot(aus_holidays, level = NULL) + xlab("Year") +
  ylab("Overnight trips (millions)") +
  scale_color_brewer(type = "qual", palette = "Dark2")

ETS(y ~ error("M") + trend("Ad") + season("M"))

sth_cross_ped <- pedestrian %>%
  filter(Sensor == "Southern Cross Station", yearmonth(Date) == yearmonth("2016 July")) %>%
  index_by(Date) %>%
  summarise(Count = sum(Count))
sth_cross_ped %>%
  model(hw = ETS(Count ~ error("M") + trend("Ad") + season("M"))) %>%
  forecast(h = "2 weeks") %>%
  autoplot(sth_cross_ped)

```


## Estimation and model selection

```{r Estimation and model selection, fig.width= 7, fig.height= 4}

aus_holidays <- tourism %>%
  filter(Purpose == "Holiday") %>%
  summarise(Trips = sum(Trips))
fit <- aus_holidays %>%
  model(ETS(Trips))
report(fit)

components(fit) %>%
  autoplot() +
  ggtitle("ETS(M,N,M) components")

residuals(fit)
residuals(fit, type = "response")

```


## Forecasting with ETS models

```{r Forecasting with ETS models, fig.width= 7, fig.height= 4}

fit %>%
  forecast(h = 8) %>%
  autoplot(aus_holidays) +
  ylab("Domestic holiday visitors in Australia (thousands)")


```


# Arima Models

## Stationarity and differencing

```{r Stationarity and differencing, fig.width= 7, fig.height= 4}

google_2015 %>% model(NAIVE(Close)) %>% gg_tsresiduals()

google_2015 %>% features(Close, feat_acf)
google_2015 %>% ACF(Close) %>% autoplot()
google_2015 %>%
  mutate(diff_close = difference(Close)) %>% ACF(diff_close) %>% autoplot()

#  null hypothesis of independence in a given time series
google_2015 %>%
  mutate(diff_close = difference(Close)) %>%
  features(diff_close, ljung_box, lag = 10)
# p-value of 0.637 => the daily change in the Google stock price is essentially 
#       a random amount which is uncorrelated with that of previous days

PBS %>%
  filter(ATC2 == "H02") %>%
  summarise(Cost = sum(Cost)/1e6) %>%
  transmute(
    `Sales ($million)` = Cost,
    `Log sales` = log(Cost),
    `Annual change in log sales` = difference(log(Cost), 12),
    `Doubly differenced log sales` = difference(difference(log(Cost), 12), 1)
  ) %>%
  gather("Type", "Sales", !!!syms(measured_vars(.)), factor_key = TRUE) %>%
  ggplot(aes(x = Month, y = Sales)) +
  geom_line() +
  facet_grid(vars(Type), scales = "free_y") +
  labs(title = "Corticosteroid drug sales", x = "Year", y = NULL)

# null hypothesis is that the data are stationary
#  => small p-values (e.g. < 0.05) suggest that differencing is required
google_2015 %>%
  features(Close, unitroot_kpss)
#  null hypothesis is rejected. That is, the data are not stationary


google_2015 %>%
  mutate(diff_close = difference(Close)) %>%
  features(diff_close, unitroot_kpss)
#  null hypothesis can not be rejected => the differenced data are stationary

aus_total_retail <- aus_retail %>%
  summarise(Turnover = sum(Turnover))
aus_total_retail %>%
  mutate(log_turnover = log(Turnover)) %>%
  features(log_turnover, unitroot_nsdiffs)    # seasonal differencing

aus_total_retail %>%
  mutate(log_turnover = difference(log(Turnover), 12)) %>%
  features(log_turnover, unitroot_ndiffs)

```

```{r eval = FALSE}
data_monthly %>% features(count, unitroot_nsdiffs)
#  returns 1 => one seasonal difference is required for stationarity

data_monthly %>% 
  mutate(diff_lag_12_count = difference(count, 12)) %>% 
  features(diff_lag_12_count, unitroot_ndiffs)
# returns 0 =>  no further one step differencing required for stationarity
# 

data_monthly %>% ACF(count) %>% autoplot()
data_monthly %>%
  mutate(diff_count = difference(count, 1)) %>% ACF(diff_count) %>% autoplot()
data_monthly %>%
  mutate(diff_count = difference(count, 7)) %>% ACF(diff_count) %>% autoplot()

data_monthly %>%
  mutate(diff_count = difference(count, 12)) %>% ACF(diff_count) %>% autoplot() 
as_tibble(data_monthly) %>%
  mutate(diff_count = difference(count, 12)) %>% summarise(Sum = sum(diff_count, na.rm = TRUE))

# null hypothesis of independence in a given time series
#                 => h0 ablehnen, für p < alpha
# p-value < 0.05 => null hypothesis to be rejected 
# => data in the given time series are dependent
#    => differenced data are needed to get stationary
data_monthly %>%
  mutate(diff_count = difference(count, 12)) %>%
  features(diff_count, ljung_box, lag = 10)


```

## Non-seasonal ARIMA models

```{r Non-seasonal ARIMA models, fig.width= 7, fig.height= 4}

us_change <- readr::read_csv("https://otexts.com/fpp3/extrafiles/us_change.csv") %>%
  mutate(Quarter = yearquarter(Time)) %>%
  as_tsibble(index = Quarter)

us_change %>% autoplot(Consumption) +
  labs(x = "Year", y = "Quarterly percentage change", title = "US consumption")

fit <- us_change %>%
  model(ARIMA(Consumption ~ PDQ(0,0,0)))
# no seasonal pattern, fit a non-seasonal ARIMA model (by setting PDQ(0,0,0))
report(fit)
glance(fit)
tidy(fit)
augment(fit)

fit %>% forecast(h=10) %>% autoplot(slice(us_change, (n()-80):n()))

us_change %>% ACF(Consumption) %>% autoplot()

us_change %>% PACF(Consumption) %>% autoplot()

fit2 <- us_change %>%
  model(ARIMA(Consumption ~ pdq(3,0,0) + PDQ(0,0,0)))
report(fit2)

fit3 <- us_change %>%
  model(ARIMA(Consumption ~ PDQ(0,0,0),
              stepwise = FALSE, approximation = FALSE))

report(fit3)

```

## ARIMA modelling in R

```{r Arima in R, fig.width= 7, fig.height= 4}

elec_equip <- as_tsibble(fpp2::elecequip)
elec_dcmp <- elec_equip %>%
  model(STL(value ~ season(window="periodic"))) %>%
  components() %>%
  dplyr::select(-.model) %>%
  as_tsibble()
elec_dcmp %>%
  autoplot(season_adjust)

elec_dcmp %>%
  gg_tsdisplay(difference(season_adjust), plot_type='partial')

fit <- elec_dcmp %>%
  model(
    arima = ARIMA(season_adjust ~ pdq(3,1,1) + PDQ(0,0,0))
  )

report(fit)
fit %>% gg_tsresiduals()

augment(fit) %>%
  features(.resid, ljung_box, lag = 24, dof = 4)
fit %>% forecast() %>% autoplot(elec_dcmp)

gg_arma(fit)

```



```{r Non-seasonal ARIMA for Seasonally adjusted, eval = FALSE}
# Non-seasonal ARIMA for Seasonally adjusted data

data_dcmp <- data_monthly %>%
  model(STL(count ~ season(window="periodic"))) %>%
  components() %>%
  # select(-.model) %>%
  as_tsibble()
data_dcmp %>%
  autoplot(season_adjust)

data_dcmp %>%
  gg_tsdisplay(difference(season_adjust), plot_type='partial')

fit <- data_dcmp %>%
  model(
    arima = ARIMA(season_adjust ~ pdq(3,1,1) + PDQ(0,0,0))
  )

report(fit)
fit %>% gg_tsresiduals()

augment(fit) %>%
  features(.resid, ljung_box, lag = 24, dof = 4)

fit %>% forecast() %>% autoplot(data_dcmp) + 
  xlim(yearmonth("2010"), yearmonth("2025"))

gg_arma(fit)

```

## Seasonal ARIMA models

```{r Seasonal ARIMA models, fig.width= 7, fig.height= 4}
eu_retail <- as_tsibble(fpp2::euretail)
eu_retail %>% autoplot(value) + ylab("Retail index") + xlab("Year")

eu_retail %>% gg_tsdisplay(difference(value, 4), plot_type='partial')

eu_retail %>% gg_tsdisplay(value %>% difference(4) %>% difference(),
  plot_type='partial')

fit <- eu_retail %>%
  model(arima = ARIMA(value ~ pdq(0,1,1) + PDQ(0,1,1)))
fit %>% gg_tsresiduals()

fit %>% forecast(h=12) %>% autoplot(eu_retail)

eu_retail %>%
  model(ARIMA(value))

h02 <- tsibbledata::PBS %>%
  filter(ATC2 == "H02") %>%
  summarise(Cost = sum(Cost)/1e6)
h02 %>%
  mutate(log(Cost)) %>%
  gather() %>%
  ggplot(aes(x = Month, y = value)) +
  geom_line() +
  facet_grid(key ~ ., scales = "free_y") +
  xlab("Year") + ylab("") +
  ggtitle("Cortecosteroid drug scripts (H02)")

h02 %>% gg_tsdisplay(difference(log(Cost), 12), plot_type='partial')

fit <- h02 %>%
  model(ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,2)))
report(fit)

augment(fit) %>%
  features(.resid, ljung_box, lag = 36, dof = 6)

h02 %>%
  model(ARIMA(log(Cost) ~ 0 + pdq(3,0,1) + PDQ(0,1,2))) %>%
  forecast() %>%
  autoplot(h02) +
    ylab("H02 sales (million scripts)") + xlab("Year")

```

```{r Seasonal ARIMA for Data_Monthly, eval = FALSE}
# Seasonal ARIMA

data_monthly %>% autoplot(count)

# data are non-stationary, with seasonality 
# => first take a seasonal difference
data_monthly %>% gg_tsdisplay(difference(count, 12), plot_type='partial')

# if seasonally differenced data appear to be non-stationary
# => take an additional first seasonal difference
#    note: for data_monthly it's worthening
data_monthly %>% gg_tsdisplay(count %>% difference(12) %>% difference(),
  plot_type='partial')

# find an appropriate ARIMA model based on the ACF and PACF
fit <- data_monthly %>%
  model(arima = ARIMA(count ~ pdq(0,1,2) + PDQ(0,1,2)))
fit %>% gg_tsresiduals(lag_max=36)

report(fit)   # AICc=7559.2 smaller => better for pdq(0,1,2) instead pdq((0,1,1)

augment(fit) %>%
  features(.resid, ljung_box, lag = 36, dof = 6)
# There are a few significant spikes in the ACF
# but the model does not fails the Ljung-Box test (P = 0.228 > 0.05)

fit %>% forecast(h=144) %>% autoplot(data_monthly) + 
  xlim(yearmonth("2010"), yearmonth("2025"))


data_monthly %>%       #  automatically select pdq() and PDQ()
  model(ARIMA(count))  #  => <ARIMA(2,0,0)(1,1,0)[12]>

fit <- data_monthly %>%
  model(arima = ARIMA(count ~ pdq(2,0,0) + PDQ(1,1,0)))
fit %>% gg_tsresiduals()
report(fit)   #     AICc=8219 worsening !! => increasing connfidence ranges
              # old AICc=7559.2 better W/ pdq(0,1,2) $PDQ((0,1,1)

fit %>% forecast(h=144) %>% autoplot(data_monthly) + 
  xlim(yearmonth("2010"), yearmonth("2025"))


```

## Arima vs ETS

```{r Arima vs ETS, fig.width= 7, fig.height= 4}

## Comparing ARIMA() and ETS() on non-seasonal data
aus_economy <- global_economy %>% filter(Code == "AUS") %>%
  mutate(Population = Population/1e6)

aus_economy %>%
  slice(-n()) %>%
  stretch_tsibble(.init = 10) %>%
  model(
    ETS(Population),
    ARIMA(Population)
  ) %>%
  forecast(h = 1) %>%
  accuracy(aus_economy)

aus_economy %>%
  model(ETS(Population)) %>%
  forecast(h = "5 years") %>%
  autoplot(aus_economy)

## Comparing ARIMA() and ETS() on seasonal data
# Consider the cement data beginning in 1988
cement <- aus_production %>%
  filter(year(Quarter) >= 1988)

# Use 20 years of the data as the training set
train <- cement %>%
  filter(year(Quarter) <= 2007)

fit_arima <- train %>% model(ARIMA(Cement))
report(fit_arima)

gg_tsresiduals(fit_arima, lag_max = 16)
augment(fit_arima) %>%
  features(.resid, ljung_box, lag = 16, dof = 5)

fit_ets <- train %>% model(ETS(Cement))
report(fit_ets)

gg_tsresiduals(fit_ets, lag_max = 16)
augment(fit_ets) %>%
  features(.resid, ljung_box, lag = 16, dof = 6)

# Generate forecasts and compare accuracy over the test set
bind_rows(
  fit_arima %>% accuracy(),
  fit_ets %>% accuracy(),
  fit_arima %>% forecast(h = "2 years 6 months") %>%
    accuracy(cement),
  fit_ets %>% forecast(h = "2 years 6 months") %>%
    accuracy(cement)
)

# Generate forecasts from an ETS model
cement %>% model(ETS(Cement)) %>% forecast(h="3 years") %>% autoplot(cement)

```



```{r data_monthly Arima vs ETS, eval = FALSE, fig.width= 7, fig.height= 4}

## Comparing ARIMA() and ETS() on seasonal data
# Consider the data beginning in 1901
data_20_century <- data_monthly %>%
  filter(year(Year_Month) >= 1901)

# Use first 30 years of the data as the training set
# train <- data_20_century  %>%
#   filter(year(Year_Month) <= 1930)
train <- data_20_century  %>%
  filter(year(Year_Month) >= 1971 & year(Year_Month) <=2000 )

# fit_arima <- train %>% model(ARIMA(count))
# automatically select pdq() and PDQ() provides 
# Model: ARIMA(0,0,1)(1,1,1)[12] w/ AICc=1406.62
fit_arima <- train %>% model(arima = ARIMA(count ~ pdq(0,1,2) + PDQ(0,1,2)))
# Model: ARIMA(0,1,2)(0,1,2)[12] w/ AICc=1397.99
report(fit_arima)

gg_tsresiduals(fit_arima, lag_max = 24) + ggtitle("fit_arima")
augment(fit_arima) %>%
  features(.resid, ljung_box, lag = 24, dof = 5)

# fit_ets <- train %>% model(ETS(count))
# automatically select ETS() provides  
# Model: ETS(A,N,A) w/ AICc 2515.437
fit_ets  <- train %>%
  model(ETS(count ~ error("A") + trend("A") + season("A")))
# Model: ETS(A,A,A) w/ AICc 2521.208
report(fit_ets)

gg_tsresiduals(fit_ets, lag_max = 24) + ggtitle("fit_ETS")
augment(fit_ets) %>%
  features(.resid, ljung_box, lag = 24, dof = 6)

# Generate forecasts and compare accuracy over the test set
bind_rows(
  fit_arima %>% accuracy(),
  fit_ets %>% accuracy(),
  fit_arima %>% forecast(h = "10 year") %>%
    accuracy(data_20_century),
  fit_ets %>% forecast(h = "10 years") %>%
    accuracy(data_20_century)
)
# output evaluates the forecasting performance of the two competing models
#   over the test set. ARIMA model seems to be the slightly more accurate model
#   based on the smaller values of test set RMSE, MAPE (here: INF) and MASE
#   
# ETS model fits the training data slightly better than the ARIMA model, 
# but that the ARIMA model provides more accurate forecasts on the test set
# A good fit to training data is never an indication that the model will 
# forecast well. 
# 
# Below we generate and plot forecasts from an ETS model for the next 10 years.
# Generate forecasts from an ETS model
data_20_century  %>% model(ARIMA(count)) %>% forecast(h="10 years") %>%
  autoplot(data_20_century) +
  xlim(yearmonth("2010"), yearmonth("2030")) +
  labs(title = "Forecast with ARIMA Model")

data_20_century %>% model(ETS(count)) %>% forecast(h="10 years") %>%
  autoplot(data_20_century) +
  xlim(yearmonth("2010"), yearmonth("2030")) +
  labs(title = "Forecast with ETS Model")

data_20_century  %>% 
  model(arima = ARIMA(count ~ pdq(0,1,2) + PDQ(0,1,2))) %>% 
  forecast(h="10 years") %>%
  autoplot(data_20_century) +
  xlim(yearmonth("2010"), yearmonth("2030")) +
  labs(title = "Forecast with fixed Model ARIMA(0,1,2)(0,1,2)[12]")

data_20_century %>% 
  model(ETS(count ~ error("A") + trend("A") + season("A"))) %>% 
  forecast(h="10 years") %>%
  autoplot(data_20_century) +
  xlim(yearmonth("2010"), yearmonth("2030")) +
  labs(title = "Forecast with fixed Model ETS(A,A,A)")
```

# Dynamic Regression Models

```{r Dynamic Regression Models, fig.width= 7, fig.height= 4}

ARIMA(y ~ x + pdq(1,1,0))

us_change <- readr::read_csv("https://otexts.com/fpp3/extrafiles/us_change.csv") %>%
  mutate(Time = yearquarter(Time)) %>%
  as_tsibble(index = Time)
us_change %>%
  gather("var", "value", Consumption, Income) %>%
  ggplot(aes(x = Time, y = value)) +
  geom_line() +
  facet_grid(vars(var), scales = "free_y") +
  xlab("Year") + ylab(NULL) +
  ggtitle("Quarterly changes in US consumption and personal income")

fit <- us_change %>%
  model(ARIMA(Consumption ~ Income))
report(fit)

bind_rows(
  `Regression Errors` = residuals(fit, type="regression"),
  `ARIMA Errors` = residuals(fit, type="innovation"),
  .id = "type"
) %>%
  ggplot(aes(x = Time, y = .resid)) +
  geom_line() +
  facet_grid(vars(type), scales = "free_y") +
  xlab("Year") + ylab(NULL)

fit %>% gg_tsresiduals()

augment(fit) %>%
  features(.resid, ljung_box, dof = 5, lag = 8)
```

## Forecasting

```{r}
us_change_future <- new_data(us_change, 8) %>% mutate(Income = mean(us_change$Income))
forecast(fit, new_data = us_change_future) %>%
  autoplot(us_change) + xlab("Year") +
  ylab("Percentage change")

vic_elec_daily <- vic_elec %>%
  filter(year(Time) == 2014) %>%
  index_by(Date = date(Time)) %>%
  summarise(
    Demand = sum(Demand)/1e3,
    Temperature = max(Temperature),
    Holiday = any(Holiday)
  ) %>%
  mutate(Day_Type = case_when(
    Holiday ~ "Holiday",
    wday(Date) %in% 2:6 ~ "Weekday",
    TRUE ~ "Weekend"
  ))

vic_elec_daily %>%
  ggplot(aes(x=Temperature, y=Demand, colour=Day_Type)) +
    geom_point() +
    ylab("Electricity demand (GW)") +
    xlab("Maximum daily temperature")

fit <- vic_elec_daily %>%
  model(ARIMA(Demand ~ Temperature + Temperature^2 + (Day_Type=="Weekday")))

fit %>% gg_tsresiduals()

augment(fit) %>%
  features(.resid, ljung_box, dof = 8, lag = 14)

vic_elec_future <- new_data(vic_elec_daily, 14) %>%
  mutate(
    Temperature = 26,
    Holiday = c(TRUE, rep(FALSE, 13)),
    Day_Type = case_when(
      Holiday ~ "Holiday",
      wday(Date) %in% 2:6 ~ "Weekday",
      TRUE ~ "Weekend"
    )
  )
forecast(fit, vic_elec_future) %>%
  autoplot(vic_elec_daily) + ylab("Electricity demand (GW)")

```

## Stochastic and deterministic trends

```{r}
aus_visitors <- as_tsibble(fpp2::austa)
aus_visitors %>%
  autoplot(value) +
  labs(x = "Year", y = "millions of people",
       title = "Total annual international visitors to Australia")

fit_deterministic <- aus_visitors %>%
  model(ARIMA(value ~ trend() + pdq(d = 0)))
report(fit_deterministic)

fit_stochastic <- aus_visitors %>%
  model(ARIMA(value ~ pdq(d=1)))
report(fit_stochastic)

bind_cols(fit_deterministic, fit_stochastic) %>%
  rename(`Deterministic trend` = 1, `Stochastic trend` = 2) %>%
  forecast(h = 10) %>%
  autoplot(aus_visitors) +
  labs(x = "Year", y = "Visitors to Australia (millions)",
       title = "Forecasts from trend models")


```

## Dynamic harmonic regression

```{r}

aus_cafe <- aus_retail %>%
  filter(
    Industry == "Cafes, restaurants and takeaway food services",
    year(Month) %in% 2004:2018
  ) %>%
  summarise(Turnover = sum(Turnover))

fit <- aus_cafe %>%
  model(
    `K = 1` = ARIMA(log(Turnover) ~ fourier(K = 1) + PDQ(0,0,0)),
    `K = 2` = ARIMA(log(Turnover) ~ fourier(K = 2) + PDQ(0,0,0)),
    `K = 3` = ARIMA(log(Turnover) ~ fourier(K = 3) + PDQ(0,0,0)),
    `K = 4` = ARIMA(log(Turnover) ~ fourier(K = 4) + PDQ(0,0,0)),
    `K = 5` = ARIMA(log(Turnover) ~ fourier(K = 5) + PDQ(0,0,0)),
    `K = 6` = ARIMA(log(Turnover) ~ fourier(K = 6) + PDQ(0,0,0))
  )

fit %>%
  forecast(h = "2 years") %>%
  autoplot(aus_cafe) +
  facet_wrap(vars(.model), ncol = 2) +
  guides(colour = FALSE) +
  geom_label(
    aes(x = yearmonth("2007 Jan"), y = 4250, label = paste0("AICc = ", format(AICc))),
    data = glance(fit)
  )

```

## Lagged predictors

```{r}

insurance <- as_tsibble(fpp2::insurance, pivot_longer = FALSE)

insurance %>%
  gather("key", "value", Quotes, TV.advert) %>%
  ggplot(aes(x = index, y = value)) +
  geom_line() +
  facet_grid(vars(key), scales = "free_y") +
  labs(x = "Year", y = NULL,
       title = "Insurance advertising and quotations")


# View(insurance %>%
#   # Restrict data so models use same fitting period
#   mutate(Quotes = c(NA,NA,NA,Quotes[4:40])))

fit <- insurance %>%
  # Restrict data so models use same fitting period
  mutate(Quotes = c(NA,NA,NA,Quotes[4:40])) %>%
  # Estimate models
  model(
    lag0 = ARIMA(Quotes ~ pdq(d = 0) + TV.advert),
    lag1= ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert)),
    lag2= ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert) + lag(TV.advert, 2)),
    lag3= ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert) + lag(TV.advert, 2) + lag(TV.advert, 3))
  )

glance(fit)

fit_best <- insurance %>%
  model(ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert)))
report(fit_best)

insurance_future <- new_data(insurance, 20) %>%
  mutate(TV.advert = 8)

fit_best %>%
  forecast(insurance_future) %>%
  autoplot(insurance) + ylab("Quotes") +
  ggtitle("Forecast quotes with future advertising set to 8")




```

## Exercises

```{r}

advert <- as_tsibble(fma::advert, pivot_longer = FALSE)
advert %>%
  model(ARIMA(sales ~ advert + pdq(0,0,0)))

vic_elec_daily <- vic_elec %>%
  filter(year(Time) == 2014) %>%
  index_by(Date = date(Time)) %>%
  summarise(
    Demand = sum(Demand)/1e3,
    Temperature = max(Temperature),
    Holiday = any(Holiday)) %>%
  mutate(
    Temp2 = I(pmax(Temperature-20,0)),
    Day_Type = case_when(
      Holiday ~ "Holiday",
      wday(Date) %in% 2:6 ~ "Weekday",
      TRUE ~ "Weekend"))

```



