---
title: Climate Data Visualization -
subtitle: Atmospheric $CO_2$ Concentration / Temperature / Precipitation
author: "Wolfgang Vollmer"
date: '`r Sys.Date()`'
output:
  pdf_document:
    fig_caption: no
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document: default
params:
  city: "Potsdam"
  temp_precip_both: "both"    
urlcolor: blue
papersize: a4
knit: (
  function(inputFile, encoding) { 
    city <- "Potsdam"
     
    rmarkdown::render( 
      input       = inputFile, 
      output_file = paste(substr(inputFile,1,nchar(inputFile)-4), city, 
                          Sys.Date(), sep = "_")) })
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
################################################################################
## Plot and Analyze                                                           ##
## Yearly/Monthly CO2 or Climate Temperature / Precipitation Data             ##
##                                                                            ##
##                                            Wolfgang Vollmer, January 2026  ##
################################################################################
# ~line 609 chunk "decomposition replace NAs" 
# => for Mannheim and Giessen not running => decomposition only for temperature
###

knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE,      
                      message = FALSE, fig.width = 7, fig.asp = 0.618)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
)

city  <- params$city
temp_precip_selected <- params$temp_precip_both

eval_visualization <- TRUE
eval_trend_season <- TRUE
eval_backup <- TRUE

setwd(dirname(rstudioapi::getSourceEditorContext()$path))
```

```{r Add config and data, child = 'Climate_Config.Rmd', eval=TRUE}
```

```{r further parameters for period calculations, include = FALSE}

### CLINO - Climatological normal (defined by WMO, Normalperiode)
# up to 2020: Reference Period WMO: 1961 - 1990
# as of 2021: Ref Period will change to 1991 - 2020
period <- 30 # length of CLINO periods
# span : provided by Climate_Config (CO2: 7, Temp&Precip: 30)
#      width of the rolling window in years, used for Gauss-Filterung
# span_m : provided by Climate_Config (=84 = 7*12)
#      width of the rolling window in months
# first_year & last_Year: provided by Climate_Config

data_monthly %>% as_tsibble(index = Year_Month, key = c(City, Measure))

measure_nr <- nrow(data_monthly %>% distinct(Measure))
measure <- measure_analysis
# measure <- tibble(data_monthly) %>% distinct(Measure)
# measure <- as.character(as_vector(measure))
# also provided by 'Climate_Config.Rmd':
#  measure_analysis <- as.character(unique(data_monthly_wide$Measure))

# data_monthly <- data_monthly %>%
#   mutate(Period_Time =
#            paste0((((Year-11) %/% period) * (period)) + 11, "-", (((Year-11) %/% period) * (period)) + 40)) %>%
#   mutate(Period_Time =
#            case_when(Period_Time == "2021-2050" ~ paste0("2021-", last_year),
#                      .default = Period_Time
#                      ))

# data_monthly_save <-  data_monthly
# data_monthly: add columns Period_start, Period_end and Period_Time
data_monthly <- data_monthly %>% 
  mutate(Period_start =  (((Year-11) %/% period) * (period)) + 11,
         Period_end =    (((Year-11) %/% period) * (period)) + 40)
data_monthly <- data_monthly %>% 
  mutate(Period_start = 
           case_when(Period_start < first_year ~ first_year,
                     .default = Period_start),
         Period_end =
           case_when(Period_end > 2025 ~ last_year,
                     .default = Period_end)) %>% 
   mutate(Period_Time = paste0(Period_start, "-", Period_end))

# data_yearly from Climate_Config, generated by uts_gen_yearly_seasonal_avg(data_monthly)
# data_yearly: add columns Period_start, Period_end and Period_Time
data_yearly <- data_yearly %>% 
  mutate(Period_start =  (((Year-11) %/% period) * (period)) + 11,
         Period_end =    (((Year-11) %/% period) * (period)) + 40)
data_yearly <- data_yearly %>% 
  mutate(Period_start = 
           case_when(Period_start < first_year ~ first_year,
                     .default = Period_start),
         Period_end =
           case_when(Period_end > 2025 ~ last_year,
                     .default = Period_end)) %>% 
   mutate(Period_Time = paste0(Period_start, "-", Period_end))

periods <- as_tibble(data_monthly) %>%
  ungroup() %>% 
  select(Period_Time) %>% 
  unique()

first_period <- periods[1,] %>% pull()
last_period <- periods[nrow(periods),] %>% pull()
ref_period <- periods[nrow(periods)-1,] %>% pull()
ref_period_old <- periods[nrow(periods)-2,] %>% pull()

```

# `r city` - Visualization of `r measure` Data `r first_year` - `r last_year`

## Monthly Time Plots with Rolling Mean

```{r time plot outlining replaced NAs, eval = eval_visualization, fig.asp = fig_asp_mult}

# checks if data_monthly is extended by columns Raw , Interploated & NA_replace
# 
# if (as_tibble(data_monthly) %>% summarise(nr_na = sum(!is.na(NA_replace))) > 0) 
#   { 
  # only useful and working if at least on data_monthly$NA_replace != NA
  # other helper: data_monthly$NA_replace[1] <- data_monthly$Raw[1]
# group_by() required, to avoid "continous" roll_mean (over group limit)
data_plot <- as_tibble(data_monthly) %>% 
  group_by(City, Measure)

plot_monthly <- 
  ggts_w_rol_mean(data_plot, Year_Month, y = Raw, span = span_m) +
  # geom_line(aes(y = data_monthly$NA_replace), col = "cyan", linewidth = 0.8) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free_y") +
  labs(y = y_label) +
  ggtitle(topic, subtitle = paste("with Rolling Mean over", span_m, "Months"))

############## auskommetiert, führt zum Fehler
# `summarise()` has grouped output by 'City'. You can override using the `.groups` argument.
# Fehler in if (as_tibble(data_monthly) %>% summarise(nr_na = sum(!is.na(NA_replace))) >  : 
#  Bedingung hat Länge > 1
# Zusätzlich: Warnmeldung:
# In Ops.factor(left, right) : ‘>’ ist nicht sinnvoll für Faktoren
# if any interpolated values =>  add line
test_NA_replace <-
  as_tibble(data_monthly) %>% summarise(nr_na = sum(!is.na(NA_replace)))
# test_NA_replace$nr_na
# sum(test_NA_replace$nr_na)
if (sum(test_NA_replace$nr_na) > 0) {
  plot_monthly <-
    plot_monthly +
    geom_line(aes(y = data_monthly$NA_replace), col = "cyan", linewidth = 0.8) +
    ggtitle(topic,
            subtitle = paste("with Rolling Mean over", span_m,
                             "Months (Cyan: interpolated values (if any))"))
  }

plot_monthly
```


```{r time plot with fabletools, eval = FALSE, fig.asp = fig_asp_mult}

data_monthly %>% autoplot(count) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free_y") +
  labs(y = y_label) +
  ggtitle(paste(topic, "(green: interpolated values)")) 
  
```

```{r Plot Monthly with Rolling Mean, eval = eval_visualization, fig.asp = fig_asp_mult}

# group_by() required, to avoid "continous" roll_mean (over group limit)
# rol_mean over whole time series needed for second plot otherwise shorter
data_plot <- as_tibble(data_monthly) %>%
  group_by(City, Measure) %>%
  mutate(rol_mean_long = stats::filter(count, filter = rep(1/span_m, span_m)))

years <- period

past_years <- TRUE  # tricky, since roll mean for early years to be kept
                    # and otherwise xlims and ylims has to be adapted
if (past_years) {
 x_min <- data_plot %$% max(Year_Month) - freq * years + 1 
                                            # yearmonth(last_year - years + 1)
 x_max <- data_plot %$% max(Year_Month)         # max(data_monthly$Year_Month)
} else {     # take first ten years
  x_min <- data_plot %$% min(Year_Month) # yearmonth(first_year)              
  x_max <- data_plot %$% min(Year_Month) + freq * years - 1 # data_monthly$Year_Month[years*12]
}

# y_min/y_max same value for Temeperature & Percipitation => not useful to be taken
y_min <- data_plot %$% min(count, na.rm = TRUE)
y_max <- data_plot %$% max(count, na.rm = TRUE)

# mauna_co2_plot %+% plot_data not running: rol_mean missing 
# => to be added before filtering
data_plot %<>% filter(Year_Month >= x_min &  Year_Month <= x_max)
plot_monthly <-  # ggplot_w_rol_mean( , x = "")
  ggts_w_rol_mean(data_plot, Year_Month, count, span = span_m) +
  # geom_point() +
  labs(y = y_label,
       title = paste(topic, "- Past", years, "years only")) +
      facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free_y")
plot_monthly + geom_line(aes(y = rol_mean_long), col = "red", linewidth = 1, na.rm = TRUE)
                             
```

## Annual seasonal plots with monthly breakdown

The seasonal charts show the monthly seasonal patterns, where available.

```{r monthly plot, eval = eval_visualization, fig.asp = fig_asp_mult}

ggts_season_w_smooth(data = data_monthly) + 
  facet_wrap(vars(Measure), ncol = 1,  , # vars(!!!key(data_monthly))
             scales = "free_y") +          # strip.position = "left")
  labs(y = y_label, 
       subtitle = paste(city, first_year, " - ", last_year))

```

\newpage

```{r Period and ref data monthly, eval = eval_visualization}
# eval = eval_visualization

# Calculate period years means over the year  

data_monthly_period  <- as_tibble(data_monthly) %>%
  group_by(City, Measure, Month, Period_Time, Period_end) %>%
  dplyr::summarise(count = mean(count, na.rm = TRUE)) %>%  
  ungroup()

data_monthly_ref_period <- data_monthly_period %>% 
  filter(Period_Time == ref_period) %>% 
  select(-Period_Time, -Period_end) %>% 
  rename(count_ref_period = count)
  
data_monthly_period  <- 
  left_join(data_monthly_period, 
            data_monthly_ref_period, 
            by = c("City",  "Measure", "Month")) %>% 
  group_by(Month, Measure) %>% 
  mutate(count_minus_ref = count - count_ref_period) 

```

```{r Period and ref data yearly, eval = TRUE}
# no longer used for table 1 in "Yearly mean values of past time periods"
# used for annual time plots

data_yearly_period <- as_tibble(data_yearly) %>%  
  group_by(City, Measure, Period_Time, Period_end) %>%
  dplyr::summarise(count = mean(Year_avg, na.rm = TRUE)) %>%  
  ungroup()

data_yearly_ref_period <- data_yearly_period %>% 
  filter(Period_Time == ref_period) %>% 
  select(-Period_Time, -Period_end) %>% 
  rename(count_ref_period = count)

data_yearly_period  <- 
  left_join(data_yearly_period, 
            data_yearly_ref_period,
            by = c("City",  "Measure")) %>% 
  group_by(City, Measure) %>% 
  mutate(count_minus_ref = count - count_ref_period) 

data_yearly  <- 
  left_join(data_yearly, data_yearly_ref_period,  
            by = c("City",  "Measure")) 
```


\newpage

### `r period`-year period plots with monthly breakdown - Cartesian and Polar Coordinates

```{r seasonal, eval = eval_visualization, fig.asp = 0.5}

title <- paste0("Monthly Variations of ", period, "-Year Periods")
subtitle <- paste("Periods: First", periods[1,], 
                  "/ Reference", ref_period,
                  "/ Last", periods[nrow(periods),], "\n")
# subtitle <- paste("First / Reference / Last Period:",
#                         paste(period_years, collapse = " / "))
      
y_label_delta <- 
  expression(paste("Delta ", CO[2], " Concentration / ppm to First Period"))

# !!  coord_polar doesn't support free scales => generate separate plots
# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
    data_filter <- filter(data_monthly_period, Measure == i)
    plot_monthly <- ggts_year_over_month(data_filter, period = Period_end) +
      labs(y = y_label_measure[i], 
           title = paste0(i, " - ", title), 
           subtitle = subtitle)
    ratio_x <- 4 # ratio x / y
    ratio_y <- 12 / (max(data_filter$count)-min(data_filter$count))
    ratio <- ratio_y / ratio_x # expressed as y / x
    writeLines("\n")
    print(plot_monthly) # + coord_fixed(ratio = ratio)
    writeLines("\n")
    print(plot_monthly + coord_polar("x", start = pi))  
    # pi => rotate by 180 degrees
    writeLines("\n_\n")
  }
  # }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```

### Plot Monthly Delta to Reference Period - Cartesian and Polar Coordinates

```{r seasonal adjusted, eval = eval_visualization, fig.asp = 0.5}

# !!  coord_polar doesn't support free scales => generate separate plots
# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
    data_filter <- filter(data_monthly_period, Measure == i)
    plot_monthly <- ggts_year_over_month(data_filter, period = Period_end,
                                         y = count_minus_ref) +
      labs(y = y_label_measure[i], 
           title = paste0(i, " - ", title, " (Delta to Reference)"), 
           subtitle = subtitle) 

    print(plot_monthly) # + coord_fixed(ratio = ratio))
    print(plot_monthly + coord_polar("x", start = pi)) 
    # pi => rotate by 180 degrees
  }
  #  }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```

```{r seasonal plots fabletools, eval = eval_visualization, fig.asp = 0.8 * fig_asp_mult}

data_monthly %>% ggtime::gg_season(count, labels = "none") +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free_y") +
  labs(x = "Month", 
       y = y_label,
       title = paste("Annual Seasonal Plots - Monthly", topic))


data_monthly %>%
  ggtime::gg_subseries(count) + 
    labs(x = "Month", 
       y = y_label_inv,
       # caption = y_label,
       title = paste("Monthly Subseries Plots - ", topic)) 

```
The blue horizontal lines within the seasonal subseries plot indicate the means for each month.

\newpage

## Annual `r topic`

### Annual Time Plot of `r measure`

```{r annual time plot, eval = eval_visualization, fig.asp = 0.8 * fig_asp_mult}

# plot mean temperature & precipitation with running/rolling mean

## Multiply "Precipitation" values/month by 12 to get mm/Year

#  England data and running mean: The red line is a 21-point binomial filter, 
#                      which is roughly equivalent to a 10-year running mean.
# for Global Temperature Circle see:
# https://www.metoffice.gov.uk/weather/climate-change/what-is-climate-change
# 
# span of years of moving/rolling average/mean (einfacher Gleitender Durchschnitt) 
# default: method = "convolution", sides = 2 => centred) 

# Gauss-Filterung ; Filtergewichte proportional zur Gauss’schen Normalverteilung
# default: method = "convolution", sides = 2 => centred, circular = FALSE)
gauss <- function(x, sig=1) { 1/sqrt(2*pi)/sig * exp(-(x^2)/2/sig) }
x <- seq(from = -(span-1)/2, to = (span-1)/2, by = 1)
# x <- seq(from = -span, to = span, by = 1) 
sig <- span
filter_coef <- gauss(x, sig)/sum(gauss(x, sig))  # normalization 
# sum(filter_coef)  # has to be 1

# Hanning-Window/Von-Hann-Fenster Filter Paket: ‘e1071’ 
# similar gauss/bell-shaped curve
# see also:  https://de.wikipedia.org/wiki/Fensterfunktion
# plot(filter_coef, col = "red")  # gauss
# library(e1071)
# filter_coef <- hanning.window(span)/sum(hanning.window(span))
# points(hanning.window(span)/sum(hanning.window(span)), col = "blue")

# rol_mean will be calculated by ggts_w_rol_mean(); gauss() not required
# yearly_plot_data <- data_yearly %>%   
#   mutate(rol_mean = 
#            stats::filter(Year_avg, filter = rep(1/span, span)),
#          rol_gauss = 
#            stats::filter(Year_avg, filter = filter_coef)) %>% 
#   ungroup()
# 
# yearly_plot_data

data_yearly_tbl <- as_tibble(data_yearly) %>% group_by(Measure, City) 
# to avoid rolling mean over Precip - Temp
plot_yearly <- ggts_w_rol_mean(data_yearly_tbl, Year, Year_avg, span = span) +
  geom_line(aes(y=count_ref_period),  linetype = "dashed", linewidth = 0.7) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free_y")

plot_yearly +
  labs(y = y_label,
       title = paste0("Yearly Data with ", span, 
                      "−years Rolling Mean and Regression lines"),
       subtitle = paste0("Reference Period ", 
                         ref_period, 
                         ": black dashed line"))


# * Blue:    Annual Mean Temperature / Precipitation
# * Red:     Rolling mean with span of `r span` years
# * Orange:  Rolling gaussian filtering with span of `r span` years
# * Green:   Linear Regression line
# * Darkblue: Local Polynomial Regression Fitting (Loess)
# * Black:    Reference Period `r period_years["ref"]`: Annual Mean Temperature / Precipitation

```

### Annual Seasonal Plot of `r measure`

```{r season, eval = eval_visualization, fig.asp = 0.8 * fig_asp_mult}
# 1)  seasonal values are calculated from the monthly data by averaging the three monthly values
# 2)  Seasons are Dec-Feb (DJF), Mar-May (MAM), June-Aug (JJA), Sept-Nov (SON).
# 3)  Winter: year refers to January.
# 
# 4)  Note for  ENGLAND TEMPERATURE data:  Seasonal data exist for Spring 1659 onwards..

ggts_season(data = data_yearly) +
  facet_wrap(vars(Measure), ncol = 1,  scales = "free_y") +
                                              # vars(!!!key(data_yearly))
  labs(y = y_label)

```

\newpage

# Trend and Seasonal Analysis

## Time Series Decomposition - Trend and Seasonal Components

An *additive model* would be used when the variations around the trend
do not vary with the level of the time series whereas a *multiplicative
model* would be appropriate if the trend is proportional to the level of
the time series.

Time series using an

-   additive model: $y_t = T_t + C_t + S_t + \epsilon_t$

-   multiplicative model: $y_t = T_t * C_t * S_t * \epsilon_t$

Trend / Cycle / Seasonal / Noise component\
Cyclical components is often grouped into the Trend component

For *Seasonal decomposition of time series by Loess (stlplus)* uses in
general an additive error model, it only provides facilities for
additive decompositions. It is possible to obtain a multiplicative
decomposition by first taking logs of the data.

```{r decomposition replace NAs, eval = eval_trend_season, fig.asp = 1.2}
# For Mannheim and Giessen replace eval = eval_trend_season by eval = FALSE !!
## decomposition

if (city == "Mannheim" | city == "Giessen") {
  data_monthly_save <- data_monthly
  data_monthly <- filter(data_monthly, Measure == "Temperature")
  
}

# filter(data_monthly, Measure == "Temperature")
# filter(data_monthly, Measure == "Precipitation")

# first_year <- data_monthly %$% min(year(Year_Month))
# last_year <- data_monthly %$% max(year(Year_Month))
# data_monthly <- data_monthly %>%  dplyr::select(Year_Month, count)

# only one single time series for ts() and stl() allowed

#   data_ts <- data_monthly %$% ts(count, start=c(first_year, 1), frequency = freq)
#   # t.window default: nextodd of
#   # s.window <- c(5, 7, 13, 21, 31, 61, 121, 241, 1001, 2001)
#   # (ceiling((1.5*freq) / (1-(1.5/s.window))))
#   #    s.window:   5, 7, 13, 21, 31, 61, 121    
#   # => t.window:  27 23, 21, 21, 19, 19,  19  #
#   #                        min = 19 da ceiling(1.5*12)=18 => nextodd = 19
   

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # Measure
    data_monthly_new <- tibble()  
      data_ts <- filter(data_monthly, Measure == i) %$% 
        ts(count, start=c(first_year, 1), frequency = freq)
      stlplus_data <- stlplus(data_ts, s.window=season_window, t.window=trend_window)
      
      data_monthly_stlplus <- uts_stlplus_as_tibble(stlplus_data)  %>% 
        mutate(Measure = i)
      # data_monthly_new <- bind_rows(data_monthly_stlplus, data_monthly_new) 
      
      decomp_plot <- ggts_decomp(data_monthly_stlplus %>% 
                                   filter(Year >= last_year-5))
      print(decomp_plot +
              labs(title =  paste("Seasonal Decomposition by Loess - ", i),
                   subtitle = paste("by stlplus(); Data = Trend + Seasonal + Remainder; ",
                                    "Horiz. Lines: Mean values"),
                   y = y_label_measure[i]))
    } 
    ## 'count' now NAs are replaced by values of Interpolated !!  #################
    # data_monthly <- data_monthly_new %>% 
    #   mutate(Year_Month = yearmonth(Year_Month),  # lost yearmonth() class by bind_row
    #          City = city) %>%  # lost yearmonth() by bind_rows
    #   as_tsibble(index = Year_Month, key = key)
#  }  
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

if (city == "Mannheim" | city == "Giessen") {
  data_monthly <- data_monthly_save
}

```

```{r decomp with fabletools, eval = FALSE, fig.asp = 1.2}
## decomposition - needs selected keys ! w/ ungroup() running !!
# *Seasonal decomposition of time series by Loess (STL)*
# * STL Seasonal Decomposition of Time Series by Loess
#     + window (trend|season): span (in lags) of the loess window, should be odd. 
#     + trend window: number of consecutive observations to be used when estimating the trend-cycle
#         + default: `ceiling((1.5*period m) / (1-(1.5/s.window))))` 
#             + $-> 21$ for monthly seasonal data w/ $m = 12$
#         + smaller values allow for more rapid changes & should be odd numbers
#         + trend(window = x): default x=21 (for monthly data) rsp. for season 11
#         + trend window is the number of consecutive observations to be used when estimating the
   
data_monthly %>% 
  filter(year(Year_Month) > last_year -60) %>% 
  ungroup() %>% 
  model(STL(count ~ trend(window=trend_window) + 
              season(window=season_window),
                  robust = TRUE))  %>% 
  fabletools::components() %>%
  filter(year(Year_Month) > last_year - 30)  %>%
  autoplot() + theme(legend.position = "bottom") + xlab("Year")

```

## Periodicities - Season Frequency

### Lag Plot - Differences

Lagged scatterplots, where the horizontal axis shows lagged $(k = 1,..,12)$ values 
of the time series. 
Each graph shows $y_t$ plotted against $y_{t-k}$ for different values of k.
For seasonal data the relationship is strongly positive at a lag $k=12$, 
reflecting the strong seasonality of the data. 
The strongly negative relationship is evident in the case oflag $k=6$.

```{r lag with single filter, eval = eval_trend_season}

## !! for data with more than one time series
## => filter a single time series to use `gg_lag()` or `gg_tsdisplay()`
# 
# print(n_keys(data_monthly))   # number of separate time series
# print(key_data(data_monthly)) # tibble with key name(s) and subnames
# #     Measure       .rows        
# #   <chr>         <list>       
# # 1 Precipitation <int [1,596]>
# # 2 Temperature   <int [1,596]>
# print(key(data_monthly))  # provides "group" key name;
#                              for facet_wrap use: vars(!!!key(data_monthly))

year_filter <- last_year - 15

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
  #  for (j in unique(key_data(data_monthly)[[2]])) {   # Measure
      
      plot_lag <- filter(data_monthly, year(Year_Month) >= year_filter & 
                           Measure == i) %>%   # City == i &Measure == j
        ggtime::gg_lag(count, lags = 1:12, geom = "point", arrow = TRUE) +
        labs(x = y_label_measure[i], y = "lag(x, n)",
             col = "Month",
             title ="Lag by n months - y(t) plotted against y(t-n)",
             subtitle = paste(i, " - data as of year", year_filter)) 
print(plot_lag)
    }
#  }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```
\newpage

<!-- ### ACF / PACF Correlogram -->

```{r ACF PACF plots, eval = FALSE}
# Plots covered by next chunk - Triplet 
# tsdisplay plot


## ACF Correlogram
plot_1 <- data_monthly %>% ACF(count, lag_max = 36) %>% 
  autoplot() + 
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free_y", 
             strip.position = "left") +
  ggtitle("ACF Correlogram - w/ slow decrease as the lags increase", 
          subtitle = "- due to the trend, while the 'scalloped' shape is due the seasonality") +
  labs(caption = paste("(P)ACF ", "bounds = 1.96/sqrt(nrow(data))"))
                 # round(2/sqrt(nrow(data)/2), digits = 3)))



## PACF - Partial ACF
plot_2 <- data_monthly %>% PACF(count, lag_max = 36) %>% 
  autoplot() +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free_y",
             strip.position = "left") +
  ggtitle("PACF - Partial Autocorrelation Function")

gridExtra::grid.arrange(plot_1, plot_2)
```

### Periodogram - Spectral Density Estimation of a Time Series

The spectral density characterizes the frequency content of the signal.
One purpose of estimating the spectral density is to detect any
periodicities in the data, by observing peaks at the frequencies
corresponding to these periodicities.

At frequency $\lambda = 1/12$ there is a significant peak =\> This
pattern repeats every full frequency = every 12 months / every year

The remaining peaks are random and therefore cannot be assigned
significantly.

Note: The blue dashed lines in the (P)ACF plots ((Partial) Autocorrelation Function) indicate white noise series limits. In that case 95% of the spikes lie within the dashed lines. 


```{r Triplet tsdisplay plot, eval = eval_trend_season}
# , error = TRUE
# 
## !! for data with more than one time series
## => filter a single time series to use `gg_lag()` or `gg_tsdisplay()`

## arttention: for Cottbus Plot with Spectrum throws an error =>error = TRUE !!
## - up to know only for Cottbus Precipitation (w/ Temperature running) 
## - Fehler in .$spec[, 1] : falsche Anzahl von Dimensionen

year_filter <- 2000

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) {
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {
   # for (j in unique(key_data(data_monthly)[[2]])) {
      plot_ts_spec <- data_monthly %>% 
        filter(year(Year_Month) >= year_filter & Measure == i) %>%  
        ggtime::gg_tsdisplay(count,  lag_max = 24) +  
        # plot_type = "spectrum",
        labs( x = "Year", y = y_label_measure[i], 
              title = "Time series plot with ACF and Spectrum",
              subtitle = paste(i, " - as of year", year_filter))
      print(plot_ts_spec)
      
      plot_ts_pacf <- data_monthly %>% 
        filter(year(Year_Month) >= year_filter & Measure == i) %>%  
        ggtime::gg_tsdisplay(count, 
                             plot_type = "partial", lag_max = 24) + 
        labs( x = "Year", y = y_label_measure[i], 
              title = "Time series plot with ACF and PACF",
              subtitle = paste(i, " - as of year", year_filter))
      print(plot_ts_pacf)
    }
#  } 
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```
\newpage


<!-- ### Seasonal vs non Seasonal ACF / Strength (Seasonal/Trend) -->

<!-- 
-   Check acf1 and season_acf1 and compare with ACF Correlogram Plot

-   acf1: first autocorrelation coefficient from the original data

-   acf10: sum of square of the first ten autocorrelation coefficients
    from the original data

-   diff1_acf1: first autocorrelation coefficient from the differenced
    data

-   season_acf1: autocorrelation coefficient at the first seasonal lag

-   Check Trend & Seasonal Strength close to 0 / 1 : weak / strong and
    compare them

-   stl_e\_acf1: first autocorrelation coefficient of the remainder
    series

-   stl_e\_acf10: sum of squares of the first ten autocorrelation
    coefficients of the remainder series

-   linearity: linearity of the trend component of the STL
    decomposition. It is based on the coefficient of a linear regression
    applied to the trend component

-   curvature: curvature of the trend component of the STL
    decomposition. It is based on the coefficient from an orthogonal
    quadratic regression applied to the trend component. 
-->

```{r seasonal vs non seasonal, eval = FALSE}

print("Check acf1 and season_acf1 and compare with ACF Correlogram Plot")
data_monthly %>% features(count, feat_acf)

print("Check Trend & Seasonal Strength close to 0 / 1 : weak / strong and compare them")
data_monthly %>% features(count, feat_stl)

plot_1 <- data_monthly %>% features(count, feat_acf) %>%
  ggplot(aes(x=acf1, y=season_acf1, col=Measure)) +
  geom_point() + facet_wrap(vars(Measure)) +
  theme(legend.position = "none") +
  ggtitle("Seasonal ACF lag1 vs. ACF lag1")
plot_2 <- data_monthly %>% features(count, feat_stl) %>%
  ggplot(aes(x=trend_strength, y=seasonal_strength_year, col=Measure)) +
  geom_point() + facet_wrap(vars(Measure)) +
  theme(legend.position = "none") +
  ggtitle("Seasonal vs. Trend Strength")
gridExtra::grid.arrange(plot_1, plot_2)


```

<!-- 
### Spectral Entropy Test 
-   Entropy close to 0 =\> series has strong trend and seasonality (=\>
    easy to forecast)
-   Entropy close to 1 =\> series is very noisy (and so is difficult to
    forecast)
-->


```{r Entropy test, eval = FALSE}
# (Shannon) spectral entropy of a time series, which is a measure of how easy 
# the series is to forecast. 
# A series which has strong trend and seasonality (and so is easy to forecast) 
# will have entropy close to 0. 
# A series that is very noisy (and so is difficult to forecast) 
# will have entropy close to 1.

print("Check entropy close to 0 or 1")
data_monthly %>% features(count, feat_spectral) 
# entropy close to 0 => series has strong trend and seasonality (=> easy to forecast)
# entropy close to 1 => series is very noisy (and so is difficult to forecast)

```

<!-- 
## Stationary Process Test

Strict-sense stationarity / Weak (wide-sense) stationarity

Augmented Dickey-Fuller test =\> type3, a linear model with both drift
and linear trend

Trend Stationary - underlying trend (function solely of time) can be
removed, leaving a stationary process
-->

```{r eval = FALSE}
## Stationarity Test
library(aTSA)     # for stationary.test(data_ts) and adf.test(data_ts) - same
# library(fractal)  # for stationaryty()
library(LaplacesDemon)

first_year<- data_monthly %$% year(min((Year_Month)))
# first_year <- as.integer(as_tibble(data_monthly) %>% 
#                            summarise(first_year = min(Year)))
freq <- 12
# data <- data_monthly_stlplus  # w/ numeric "Time"
# as.numeric(as.Date("2018-09-24") - as.Date("2017-10-24"))
# update data_ts with replaced NAs, e.g. adf.test() does not allow NAs
data_ts <- ts(data_monthly$count, start=c(first_year, 1), frequency = freq)

# Performs stationary test for a univariate time series
# combines the existing functions adf.test, pp.test and kpss.test 
# for testing the stationarity of a univariate time series x.
# null hypothesis of a unit root of a univarate time series x 
# (equivalently, x is a non-stationary time series)
# => p > 0.05 h0 can not be rejected  => non-stationary
# cat("Null Hypothesis of non-stationary time series - for p < 0.05: reject H_0")
# stationary.test(data_ts) 

# already covered by unitroot_kpss test
# data_monthly %>% 
#   features(count, list(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))

# The results are the same as one of the adf.test, pp.test, kpss.test, depending on which test 
# for MAuna Loa: adf.test: 

# adf.test(data_ts)  # Augmented Dickey-Fuller test

# kpss.test(data_ts) # KPSS Unit Root Test 
# pp.test(data_ts)   # Phillips-Perron Unit Root Test

# Priestley-Subba Rao (PSR) test
# statio <-
#   stationarity(data_monthly$count, n.taper = 6,
#                n.block = max(c(12, floor(logb(length(data_ts), base = 12)))),
#                significance = 0.05, center = TRUE, recenter = FALSE)
# summary(statio)

is.stationary(data_monthly$count)
# statcheck() from Schlittgen
# source("./Data_Schlittgen/tsutil.R")
# statcheck(data_monthly$count, 5) #Berechnung der deskriptiven Maßzahlen ist elementar. 
     # Um die Kovarianzstationarität zu überprüfen, wird die Funktion eingesetzt
     #  plot ACF over Lag  for 5 segments
#  function statcheck determines the means, standard deviations and acf’s
#  of segments of a time series and plots the acf’s for the segments.

# already covered by unitroot_ndiffs, unitroot_nsdiffs tests
# data_monthly %>% 
#   features(count, list(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))
# vartable(data_ts,12) # variate Differenzen weisen darauf hin, dass einmal 
#                      # einfache und einmal saisonale Differenzen zu bilden sind
#                      # sind, um Stationarität zu erreichen.



# bandfilt(data_ts, 7, 12, 24)
# View(data_ts)
# 
# (model_lm_1 <- lm(count ~ Year_Month, data))
# summary(model_lm_1)
# plot(model_lm_1)
# 
# model_lm_2 <- lm(count ~ Year_Month + Year_Month^2, data)
# summary(model_lm_2)
# plot(model_lm_2)


```

```{r Beispiel 10.10, eval = FALSE}
library(dlm)                                                                                 
# co2 <- scan("./Data_Schlittgen/schauinsland.dat")  
co2 <- data_ts  # co2 <- ts(co2,start = 2005,frequency=12)
dlmco <- dlmModPoly(2) + dlmModSeas(12)
m1 <- c(382,0.1,rep(0,11))  
c1 <- diag(c( 0.1, 0.1, rep(100,11)))

buildFun <- function(x) { 
  W(dlmco)[1:3,1:3] <- diag(exp(x[1:3]))
  V(dlmco) <- exp(x[4]) 
  C0(dlmco) <- c1
  m0(dlmco) <- m1 
  return(dlmco)
}              

# Parameter estimation by maximum likelihood - very long runnning !!
fit <- dlmMLE(co2, parm=rep(1,4), build=buildFun)
fit$conv
dlm.co2 <- buildFun(fit$par)                                              
coFilter <- dlmFilter(co2, mod=dlm.co2)
coSmooth <- dlmSmooth(coFilter) 

plot(co2,type="o") 
lines(dropFirst(coFilter$m[,1]))
lines(dropFirst(coSmooth$s[,1]))
plot(dropFirst(coSmooth$s[,3]))

fut1 <- dlmForecast(coFilter, n=12)
x <- cbind(co2,fut1$f,fut1$f-1.96*sqrt(unlist(fut1$Q)),
           fut1$f+1.96*sqrt(unlist(fut1$Q)))                                
plot.ts(x,plot.type="s",lty=c(1,1,2,2)) 
points(2005+28/12,co2[29])




```

\newpage

# Forecasting - Estimate/Train the model

##  Forecasting with ETS and ARIMA model

**E**xponen**T**ial **S**moothing (**ETS**) and **A**uto**R**egressive **I**ntegrated **M**oving **A**verage Forecasting Models **ARIMA** models are the two most 
widely used approaches to time series forecasting, and provide complementary 
approaches to the problem.

Forecasts produced using **ETS** methods are weighted averages of past observations, with the weights decaying exponentially 
as the observations get older. 

Here a **$ETS(A,A|A)$ model** with additive (“A”) *Error term*, *Trend term* and 
*Seasonal term* was chosen.

While exponential smoothing models are based on a description of the trend and 
seasonality in the data, **ARIMA** models aim to describe the autocorrelations in the data.

Here a **$ARIMA(111)(011)_{12}$ model** with autoregressive, differencing, and moving average terms of (111) in the ordinary and 011 in the seasonal term with a seasonal period 12 (12 months/year)
```{r workflow estimation}

data_monthly <- data_monthly |>
  as_tsibble(index = Year_Month, key = c(City, Measure))

data_monthly_range <-  data_monthly |>
  filter(year(Year_Month) > last_year - 60) %>% 
  select(City, Measure, Year_Month, Year, Month, count)

# Estimate/Train the model
fit <- data_monthly|>
  filter(year(Year_Month) > last_year - 60) %>%  
  model(
    ETS_AAA = ETS(count ~ error("A") + trend("A") + season("A")),
    arima_111_011 = ARIMA(count ~ pdq(1,1,1) + PDQ(0,1,1)),
    )
fit

# fit %>%
#     pivot_longer(3:ncol(fit), names_to = "Model name",
#                  values_to = "Orders")

```


```{r workflow evaluation report}

fit_ets <- fit %>% select(Measure, ETS_AAA)
fit_arima <- fit %>% select(Measure, arima_111_011)

# for(i in 1:measure_nr) {
#   print(city_measure[i])
#   report(fit_ets %>% filter(Measure == measure[i]))
#   report(fit_arima %>% filter(Measure == measure[i]))
# }

# tidy(fit)
```


```{r workflow forecast}
# Forecast
# print('forecast(fit, h = "10 years")')
# forecast(fit, h = "10 years")
# print('forecast(fit, h = "10 years") |> hilo()')
# forecast(fit, h = "10 years") |> hilo() # w/ (80% & 95% prediction interval values)

forecast(fit, h = "10 years") |>
  autoplot(data_monthly %>% filter(year(Year_Month) > last_year - 30)) +   
  facet_wrap(vars(Measure), ncol = 1, scales = "free_y", strip.position = "left") +
  labs(title = "Monthly Climate Data w/ Forecast with fit on data set of past 60 years",
       y="count")
forecast(fit, h = "10 years") |>
  autoplot(data_monthly %>% filter(year(Year_Month) > last_year - 10)) +   
  facet_wrap(vars(Measure), ncol = 1, scales = "free_y", strip.position = "left") +
  labs(title = paste("Monthly Climate Data w/ Forecast (plot as of ", last_year-9, ")"),
       y="count")


```

\newpage

##  Forecast Accuracy Evaluation

* Forecast Accuracy Evaluation w/ training data "data_train" & test data "data_test"
    + "data" : complete dataset includes the forecasted (future) data range on top of data_train
    + "data_train" = "data" - forecast_range ("data_test")
        +  data used to train the model (~80% of "data)
    + "data_test" = "data" - "data_train"
        + ~ 20% of "data"
    + e.g. for last_year = 2025:
        + data_train is selected from 1966 - 2015
        + data_test is selected from 2016 - 2025

```{r workflow forecast accuracy}

# Forecast Accuracy Evaluation w/ training data "data_train"
data_train <- data_monthly |>
  filter(year(Year_Month) > last_year - 60) %>%  
  filter(year(Year_Month) <= last_year -10)
# for last_year = 2025:
# data_train from 1966 - 2015
# data_test from 2016 - 2025

fit_train <- data_train |>
  model(
    ETS_AAA = ETS(count ~ error("A") + trend("A") + season("A")),
    # ETS_MAA = ETS(count ~ error("M") + trend("A") + season("A")),
    arima_111_011 = ARIMA(count ~ pdq(1,1,1) + PDQ(0,1,1))
    )
# fit_train 

fc_data_train <- fit_train %>% forecast(h = "10 years") 

# with further data filtering only xlims are adapted, no forecast impact
# level = prediction interval in % or NULL
fc_data_train %>%
  autoplot(filter(data_monthly, year(Year_Month) > last_year - 30)) +  
           #, level = NULL) + # level = prediction interval in % or NULL
  facet_wrap(vars(Measure), ncol = 1, scales = "free_y", strip.position = "left") +
  labs(y = "Count CO2 / Temp / Precip", 
       title ="Accuracy of Monthly Forecasts", 
       subtitle = "w/ Forecast on trainings data set compared w/ real data") +
  guides(colour=guide_legend(title="Forecast"))

# accuracy(fc_data_train, data_monthly %>% 
#            filter(year(Year_Month) > last_year - 60)) %>% 
#   arrange(RMSE)
```


\newpage

### components(fit_ets) - plot of the decomposition of the fitted ETS model

* Note: compare Time series decomposition, for ETS model is valid:
    + count = lag(level, 1) + lag(slope, 1) + lag(season, 12) + remainder

```{r workflow ETS_AAA fit decomposition}

for(i in 1:measure_nr) {
  print(measure[i])
  plot <- components(fit_ets) |> filter(Measure == measure[i]) %>%
    filter(year(Year_Month) > last_year - 30)  %>% 
    autoplot()
  print(plot)
}
```


\newpage

### gg_tsresiduals(fit) - plot of innovation residuals, acf and histogram

* gg_tsresiduals(fit) (Ch 7.3 Evaluating the regression model)    
    + TS of innovation residuals, acf plot, histogram of residuals | PACF (plot_type='partial') 
    + innovation residuals should have constant variance (“homoscedasticity”)
    + histogram of the innovation residuals: should be normally distributed

```{r workflow evaluation gg_tsresiduals, eval = TRUE}

for(i in 1:measure_nr) {
  # print(measure[i])
  fit_ets_sel<- 
    fit_ets %>% filter(Measure == measure[i]) %>% slice(1) %>% select(2) %>% pull()
  fit_arima_sel<- 
    fit_arima %>% filter(Measure == measure[i]) %>% slice(1) %>% select(2) %>% pull()
  plot_ets <- fit_ets %>% filter(Measure == measure[i]) %>% 
    ggtime::gg_tsresiduals(lag_max = 24) + 
    # lag = 2*m (lenght of seasonal period), w/o seasonal: use lag = 10
    labs(title = paste(measure[i], "- Residuals for fit model ", fit_ets_sel))
  print(plot_ets)
    plot_arima <- fit_arima %>% filter(Measure == measure[i]) %>% 
    ggtime::gg_tsresiduals(lag_max = 24) + 
    # lag = 2*m (lenght of seasonal period), w/o seasonal: use lag = 10
    labs(title = paste(measure[i], "- Residuals for fit model ", fit_arima_sel))
  print(plot_arima)
}

```



\newpage

# Forecast Tables

## Yearly mean values of past time periods

```{r mean data table of past time periods, eval = TRUE}

# data_yearly_period <- tibble(data_yearly) %>% group_by(City, Measure, Period_Time) %>% 
#   summarise(Mean = mean(count))
data_yearly_monthly <- tibble(data_monthly) %>% group_by(City, Measure, Year, Period_Time) %>% 
  summarise(Mean = mean(count)) 
data_yearly_monthly_period <- data_yearly_monthly %>% group_by(City, Measure, Period_Time) %>% 
  summarise(Period_Mean = mean(Mean)) 

data_monthly_new_period <- tibble(data_monthly) %>% group_by(City, Measure, Month, Period_Time) %>% 
  summarise(count = mean(count)) 



data_yearly_monthly_period_wide <- data_yearly_monthly_period %>% 
  ungroup() %>% select(-City) %>% 
  pivot_wider(names_from = Measure,
              values_from = Period_Mean)

knitr::kable(data_yearly_monthly_period_wide, digits = 1, 
             caption = "Mean values for the given time periods; 
Units: Temperature (degree C), Precipitation (mm/Month), CO2 (ppm)")

```

## Yearly mean forecast values for the next 25 years

```{r ETS ARIMA Forecast Table yearly mean of monthly data}

cond <- c(last_year + 1, seq.int(2030, 2050, 5))
Year.x = last_year + 1 
Year.y = 2050

fc_monthly_ets_arima <- fit %>% fabletools::forecast(h = "25 years")
fc <- fc_monthly_ets_arima %>% 
  mutate(Year = year(Year_Month)) %>% 
  group_by(City, Measure, .model, Year)

data_monthly_fc <- tibble(fc) %>% 
  group_by(City, Measure, .model, Year)

data_yearly_fc <- tibble(fc) %>% 
  group_by(City, Measure, .model, Year) %>% 
  summarise(Yearly_Mean = mean(.mean))

data_yearly_fc_wide <- data_yearly_fc %>% 
  pivot_wider(names_from = .model,
              values_from = Yearly_Mean)

data_filter_fc <- filter(data_yearly_fc_wide,
                         Year == cond[1] | Year == cond[2] | 
                         Year == cond[3] | Year == cond[4] | 
                         Year == cond[5] | Year == cond[6])

knitr::kable(data_filter_fc, digits = 2, 
             caption = "Mean Yearly ARIMA and ETS Forecast values (next 25 years);
Units: Temperature (degree C), Precipitation (mm/Month), CO2 (ppm)")

```


```{r ETS ARIMA Yearly Forecast of monthly data Table delta}

x <- data_yearly_fc %>% filter(Year == Year.x) %>% rename(Mean = Yearly_Mean)
y <- data_yearly_fc %>% filter(Year == Year.y) %>% rename(Mean = Yearly_Mean)

z <- left_join(x, y, join_by(Measure, .model)) %>% 
  mutate(Delta = Mean.y - Mean.x)

z_wide <- z %>% 
  pivot_wider(names_from = .model,
              values_from = c(Mean.x, Mean.y, Delta)) %>% 
  rename(ETS.x = Mean.x_ETS_AAA, 
         ARIMA.x = Mean.x_arima_111_011,
         ETS.y = Mean.y_ETS_AAA, 
         ARIMA.y = Mean.y_arima_111_011,
         Delta_ETS = Delta_ETS_AAA, 
         Delta_ARIMA = Delta_arima_111_011)

data_z_wide <- z_wide %>% ungroup() %>% 
  select(Measure,  
         Year.x, Year.y, 
         ETS.x, ARIMA.x, ETS.y, ARIMA.y, Delta_ETS, Delta_ARIMA)

knitr::kable(data_z_wide, digits = 2, 
             caption = "Forecast increase/decrease over the next 25 years;
Units: Temperature (degree C), Precipitation (mm/Month), CO2 (ppm)")

```



```{r ETS ARIMA Monthly Forecast Table delta, eval = TRUE}
# for ETS model in principle no monthly delta difference
# for ARIMA model very small differences only => Yearly Forecast Table delta suficient 

data_monthly_fc <- data_monthly_fc %>% 
  mutate(
    # get Months in English, replace Mrz, ..., Okt, Dez by Mar, Oct, Dec)
    Year = year(Year_Month),
    Month = month(Year_Month)) %>%
    mutate(Month = factor(Month)) %>% 
  ungroup() %>% 
  select(-count, -Year_Month)
levels(data_monthly_fc$Month) <- month.abb

# note: Year.x = 2026 and Year.y = 2050
x_m <- data_monthly_fc %>% filter(Year == Year.x)  %>%
  rename(Mean.x = .mean)
y_m <- data_monthly_fc %>% filter(Year == Year.y)  %>%
  rename(Mean.y = .mean)
z_m <- left_join(x_m, y_m, join_by(City, Measure, .model, Month)) %>%
  mutate(Delta = Mean.y - Mean.x)

z_m_wide <- z_m %>% 
  pivot_wider(names_from = .model,
              values_from = c(Mean.x, Mean.y, Delta)) %>% 
  rename(Mean.x_ETS = Mean.x_ETS_AAA, 
         Mean.x_ARIMA = Mean.x_arima_111_011,
         Mean.y_ETS = Mean.y_ETS_AAA, 
         Mean.y_ARIMA = Mean.y_arima_111_011,
         Delta_ETS = Delta_ETS_AAA, 
         Delta_ARIMA = Delta_arima_111_011)

data_z_m_wide <- z_m_wide %>% 
  select(Measure,   
         Month, Year.x, Year.y, 
         Mean.x_ETS, Mean.x_ARIMA, Mean.y_ETS, Mean.y_ARIMA, Delta_ETS, Delta_ARIMA)

knitr::kable(data_z_m_wide, digits = 2, 
             caption = "Forecast increase/decrease over the next 25 years;
Units: Temperature (degree C), Precipitation (mm/Month), CO2 (ppm)")

```

\newpage

# Backup

## `r city` - Average Yearly and Seasonal Data

```{r data print, eval = eval_backup}

data_yearly_temp <- bind_rows(as_tibble(slice(data_yearly, 1:10)), 
                              as_tibble(slice(data_yearly, n() -9:0))) %>% 
  select(City, Measure, Year, 
         Winter_avg, Spring_avg, Summer_avg, Fall_avg, Year_avg )

# knitr printout in for loop not running, print(knitr()) destroys formatting
i <- 1
knitr::kable(filter(data_yearly_temp, Measure == measure[i]),
             digits = 1, caption = 
               paste("Annual", y_label_measure[measure[i]], 
                     "(first and last 10 years)"))

# in if loop knitr printout only for last knitr() statement
if (length(measure) >= 2) {
  i <- 2
  knitr::kable(filter(data_yearly_temp, Measure == measure[i]),
               digits = 1, caption = 
                 paste("Annual", y_label_measure[measure[i]], 
                       "(first and last 10 years)"))
}

# in if loop knitr printout only for last knitr() statement
if (length(measure) == 3) {
  i <- 3
  knitr::kable(filter(data_yearly_temp, Measure == measure[i]),
               digits = 1, caption = 
                 paste("Annual", y_label_measure[measure[i]], 
                       "(first and last 10 years)"))
}

```

```{r provide monthly mean over all years}

################### group_by() => index_by(time_index-group) ###################################
# index_by() is the counterpart of group_by() in temporal context, 
# but it only groups the time index => afterwards group_by_key()

# month average over all years grouped by (City, Measure, Month)
mean_monthly <- data_monthly %>%  
  as_tibble() %>%   
  group_by(City, Measure, Month) %>% 
  summarise(Month_avg = mean(count))


mean_monthly_Wide <- mean_monthly %>% 
  pivot_wider(names_from = Measure, values_from = Month_avg) 

knitr::kable(filter(mean_monthly_Wide),
             digits = 1, caption = 
               paste0("Monthly Means over all Years (", y_label_text, ")"))

```

## `r city` - Head and tail of data

```{r}
for(i in 1:measure_nr) {
  print(data_monthly %>%  filter(Measure == measure[i]) %>% 
          select(City, Measure, Year_Month, Period_Time, count) %>% 
          # select(-Raw, -NA_replace) %>% 
          slice(1:3, n()-2,n()-1,n()))
}
```


\newpage

## Data Sources

### Temperatures and Precipitation

* Basel / Davos: **Federal Office of Meteorology and Climatology
    MeteoSwiss**
    + <https://www.meteoswiss.admin.ch/home/climate/swiss-climate-in-detail/homogeneous-data-series-since-1864.html>

* `r paste(city_dwd, collapse = "/ ")`: **DWD Archiv Monats- und
    Tageswerte**
    + <https://www.dwd.de/DE/leistungen/klimadatendeutschland/klarchivtagmonat.html>
    + *Monatswerte historisch und aktuell* 
    + File: produkt_klima_monat_xy.txt
        + column MO_TT (Temperature; Monatsmittel der Lufttemperatur in 2m Höhe in \textdegree C and MO_RR (Precipitation; Monatssumme der Niederschlagshoehe in mm))

* England **Met Office - National Meteorological Service for the UK**
    + <https://www.metoffice.gov.uk/hadobs/hadcet/data/download.html>
    + Monthly_HadCET_mean.txt, 1659 to date

### CO2 Concentrations

* **National Oceanic & Atmospheric Administration - Earth System Research Laboratory**
    + *NOAA ESRL* <https://www.esrl.noaa.gov/gmd/ccgg/trends/global.html>
    + Data file: __*Mauna Loa CO2*__ monthly mean data
    + <https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html>

## R code

* Source code (maybe not yet the latest version) and output files are stored on GitHub repository  <https://github.com/WoVollmer/R-TimesSeriesAnalysis/tree/master/Climate>
* Partially based on *c't Magazin* articles by *Andreas Krause*:
    + #3/2014 p.188 <http://www.ct.de/1403188> & #6/2014 p.180 <http://www.ct.de/1406180>
* *Forecasting: Principles and Practice (3rd ed)* <https://otexts.com/fpp3>
    + Rob J Hyndman and George Athanasopoulos; Monash University, Australia

```{r ACF with ggplot, eval = FALSE}

## Test Plots

# acf() does not allow NAs and facetting
data_ts <- ts(data_monthly$count, start=c(first_year, 1), frequency = freq)

# Korrelogramm
# „empirische Autokovarianz- und Autokorrelationsfunktion (ACF)
# 
#  Lag - Verschiebung, des betrachteten Zeitfensters
#  Lag 0: Jan mit Jan (sich selbst)
#  Lag 1: Jan mit Feb (next Feb , einen Monat weiter)
#  
#  Lag 12: Jan mit Jan (des nächsten Jahres)
#  t <- c(1,2,3) => lag(t): [1] NA  1  2
lag_max <- 4 * freq
data_acf <- acf(data_ts, lag.max = lag_max, plot = FALSE, type = "correlation")
data_pacf <- acf(data_ts, lag.max = lag_max, plot = FALSE, type = "partial")
# plot(data_acf)
 
# Temperaturzeitreihe weist natürliche saisonale Schwankungen auf
# => gut im Korellogramm zu erkennen kann
# Der Januar wird als Basis gewählt und die Monate um den Januar herum weisen 
#   eine sehr hohe positive Korrelation auf
# die Sommermonate in der Mitte (0,5) des Jahres weisen eine starke
#                  negative Korrelation auf
data_acf_tbl <- tibble(ACF = c(NA, as.vector(data_acf$acf[-1])), 
    # take out redundant first value lag = 0 => always one (also taken out by ACF()) !
                       PACF = c(NA, as.vector(data_pacf$acf)), # has no lag=0 value !
                       Lag = as.vector(freq*data_acf$lag))
data_acf_tbl %<>% pivot_longer(
  cols = c("ACF", "PACF"),
  names_to = "ACF_PACF",
  values_to = "value") 
bounds <- round(2/sqrt(data_acf$n.used), digits = 3)  # data_acf$n.used = nrow(data)
ggplot(data_acf_tbl, aes(Lag, value)) +
  geom_bar(stat = "identity", width = 0.2) +  
  # geom_line() +
  geom_hline(yintercept = bounds, col = "blue", lty = "dashed") +
  geom_hline(yintercept = -bounds, col = "blue", lty = "dashed") +  
  facet_wrap( ~ ACF_PACF, ncol = 1, scales = "free_y", strip.position = "left") +
  ggtitle(paste("Time Series Correlogram"),
          subtitle = paste("with Season Frequency = ", freq, ",  lag.max = ", lag_max)) +
  labs(y = "", x = "Lag / months")

  

```

```{r eval = FALSE}
# Periodogramm
# Das Periodogramm ist das Pendant zum Korellogramm auf Spektralebene
# Es ist die abgebildete Fouriertransformation der Autokovarianzfunktion 
data_period <- 
  spec.pgram(data_ts, taper = 0, pad = 0, fast = TRUE, demean = FALSE,
             detrend = TRUE, plot = FALSE, na.action = na.fail)
#  Frequenz λ = 1: signiﬁkanter Ausschlag stattﬁndet
#  => vorliegende Muster wiederholt sich jede volle Frequenz / jedes Jahr
#  Die restlichen Ausschläge sind bereits zu zufällig, um sie signiﬁkant 
#                                                    einordnen zu können.

data_period_tbl <- as_tibble(data_period$freq) %>% 
  mutate(Frequency = value,
         Spectrum = data_period$spec)

ggplot(data_period_tbl, aes(Frequency, Spectrum)) +
  geom_line() +
  # geom_point() +
  labs(x = paste("Frequency\n", "bandwidth = ", data_period$bandwidth)) +
  scale_y_log10() +
  ggtitle( paste("Time Series", data_period$method))

```

```{r yearly data with replaced NAs, eval = FALSE}

### now with replaced NAs

data_monthly_wide <- data_monthly %>%
  as_tibble() %>% 
  pivot_wider(id_cols = c(Year, Measure),
              names_from = Month,
              values_from = count) %>% 
  mutate(City = city)

# data_yearly - generation from data_monthly_wide
#     data_check_and_fill_w_na(data_monthly,
# w/o gaps and as tsibble (no winter, ... since season = FALSE), maybe w/ NAs
# 
data_yearly <- uts_gen_yearly_seasonal_avg(data_monthly)
data_yearly <- as_tsibble(data_yearly, index = Year, key = key)


nr_series_w_gaps <- ifelse(has_gaps(data_yearly, .full = TRUE)$.gaps, 1, 0) 

if (sum(nr_series_w_gaps) > 0) {    # Check for missing years
  print(data_yearly %>% scan_gaps(.full = TRUE))  # Reveal
  print(data_yearly %>% count_gaps(.full = TRUE)) # Summarise - number of gaps from - to
  data_yearly %<>% fill_gaps(.full = TRUE)       # Fill in time gaps
  
  print(data_yearly %>% has_gaps(.full = TRUE))   # Check for gaps
}
  
```

```{r end, echo = FALSE}
###########################################################################

Prog.End <- Sys.time()
run.time <- round((Prog.End - Prog.Start), digits = 2)
message("Program executed on: ", Prog.End, "\t Program run time: ", run.time, " secs")

```
