---
title: Climate Data Visualization -
subtitle: Atmospheric $CO_2$ Concentration / Temperature / Precipitation
author: "Wolfgang Vollmer"
date: '`r Sys.Date()`'
output:
  pdf_document:
    fig_caption: no
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document: default
params:
  city: "Mauna Loa"
  temp_precip_both: "both"    
urlcolor: blue
papersize: a4

knit: (
  function(inputFile, encoding) { 
    city <- "CO2_Mauna_Loa" 
  
    rmarkdown::render( 
      input       = inputFile, 
      output_file = paste(substr(inputFile,1,nchar(inputFile)-4), city, 
                          Sys.Date(), sep = "_")) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE,      
                      message = FALSE, fig.width = 7, fig.asp = 0.618)

knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE
)

city  <- params$city
temp_precip_selected <- params$temp_precip_both

eval_visualization <- TRUE
eval_trend_season <- TRUE
eval_backup <- TRUE

setwd(dirname(rstudioapi::getSourceEditorContext()$path))
```


```{r Add config and data, child = 'Climate_Config.Rmd', eval=TRUE}
```



```{r further parameter, echo = FALSE}
######  Further Paramaters #####################################################
# define first, reference and last periods
# 
period <- span # for average temp and precip comparisons at begin & end of data

# first_year <- first_year_w_Jan_Temp
# last_year <- data_monthly %>% 
#   filter(Month == "Dec" & Measure == "Temperature" & !is.na(count)) %$% 
#   max(Year)

first_period_start <- first_year    # first year with January values
first_period_end <- first_period_start + period - 1

### CLINO - Climatological normal (defined by WMO, Normalperiode)
ref_period_start <- 1961   # Reference Period WMO: 1961 - 1990
                           # Begin 2021: Ref Period will change to 1991 - 2020
if (last_year >= 2020) ref_period_end <- 2020                         
ref_period_start <- ref_period_end - period + 1

last_period_end <- last_year      # last year with December values
last_period_start <- last_period_end - period + 1

# period_years <- c(paste(first_period_start, "-", first_period_end, sep = ""),
#           paste(ref_period_start, "-", ref_period_end, sep = ""),
#           paste(last_period_start, "-", last_period_end, sep = ""))
# names(period_years) <- c("first", "ref", "last")
```

```{r provide yearly and monthly means}

################### group_by() => index_by(time_index-group) ###################################
# index_by() is the counterpart of group_by() in temporal context, 
# but it only groups the time index => afterwards group_by_key()


# first / last year - for each group
mean_all_years <- 
  as_tibble(data_monthly) %>% 
  group_by(City, Measure) %>% 
  summarise(mean = mean(count), n = n(),
            first_year = min(Year),
            last_year = max(Year))
# mean_all_years 

# used in backup: table Month_avg over all years
# month average over all years grouped by (City, Measure, Month)
mean_monthly <- data_monthly %>%  
  as_tibble() %>%   
  group_by(City, Measure, Month) %>% 
  summarise(Month_avg = mean(count))

```

# `r city` - Visualization of `r measure` Data `r first_year` - `r last_year`

## Monthly Time Plots with Rolling Mean


```{r time plot outlining replaced NAs, eval = eval_visualization, fig.asp = fig_asp_mult}

# checks if data_monthly is extended by columns Raw , Interploated & NA_replace
# 
# if (as_tibble(data_monthly) %>% summarise(nr_na = sum(!is.na(NA_replace))) > 0) 
#   { 
  # only useful and working if at least on data_monthly$NA_replace != NA
  # other helper: data_monthly$NA_replace[1] <- data_monthly$Raw[1]
# group_by() required, to avoid "continous" roll_mean (over group limit)
data_plot <- as_tibble(data_monthly) %>% 
  group_by(City, Measure)

plot_monthly <- 
  ggts_w_rol_mean(data_plot, Year_Month, y = Raw, span = span_m) +
  # geom_line(aes(y = data_monthly$NA_replace), col = "cyan", size = 0.8) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free") +
  labs(y = y_label) +
  ggtitle(topic, subtitle = paste("with Rolling Mean over", span_m, "Months"))

# if any interpolated values =>  add line
if (as_tibble(data_monthly) %>% summarise(nr_na = sum(!is.na(NA_replace))) > 0) 
{ 
  plot_monthly <- 
    plot_monthly +
    geom_line(aes(y = data_monthly$NA_replace), col = "cyan", size = 0.8) +
      ggtitle(topic, 
              subtitle = paste("with Rolling Mean over", span_m, 
                               "Months (Cyan: interpolated values (if any))"))
}

plot_monthly
```

```{r time plot with fabletools, eval = FALSE, fig.asp = fig_asp_mult}

data_monthly %>% autoplot(count) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free") +
  labs(y = y_label) +
  ggtitle(paste(topic, "(green: interpolated values)")) 
  
```

```{r Plot Monthly with Rolling Mean, eval = eval_visualization, fig.asp = fig_asp_mult}

# group_by() required, to avoid "continous" roll_mean (over group limit)
# rol_mean over whole time series needed for second plot otherwise shorter
data_plot <- as_tibble(data_monthly) %>%
  group_by(City, Measure) %>%
  mutate(rol_mean_long = stats::filter(count, filter = rep(1/span_m, span_m)))

years <- 15

past_years <- TRUE  # tricky, since roll mean for early years to be kept
                    # and otherwise xlims and ylims has to be adapted
if (past_years) {
 x_min <- data_plot %$% max(Year_Month) - freq * years + 1 
                                            # yearmonth(last_year - years + 1)
 x_max <- data_plot %$% max(Year_Month)         # max(data_monthly$Year_Month)
} else {     # take first ten years
  x_min <- data_plot %$% min(Year_Month) # yearmonth(first_year)              
  x_max <- data_plot %$% min(Year_Month) + freq * years - 1 # data_monthly$Year_Month[years*12]
}

# y_min/y_max same value for Temeperature & Percipitation => not useful to be taken
y_min <- data_plot %$% min(count, na.rm = TRUE)
y_max <- data_plot %$%  max(count, na.rm = TRUE)

# mauna_co2_plot %+% plot_data not running: rol_mean missing 
# => to be added before filtering
data_plot %<>% filter(Year_Month >= x_min &  Year_Month <= x_max)
plot_monthly <-  # ggplot_w_rol_mean( , x = "")
  ggts_w_rol_mean(data_plot, Year_Month, count, span = span_m) +
  # geom_point() +
  labs(y = y_label,
       title = paste(topic, "- Past", years, "years only")) +
      facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free")
plot_monthly + geom_line(aes(y = rol_mean_long), col = "red", size = 1, na.rm = TRUE)
                             
```


## Yearly plots with monthly breakdown

```{r monthly plot, eval = eval_visualization, fig.asp = fig_asp_mult}

ggts_season_w_smooth(data = data_monthly) + 
  facet_wrap(vars(Measure), ncol = 1,  , # vars(!!!key(data_monthly))
             scales = "free") +          # strip.position = "left")
  labs(y = y_label, 
       subtitle = paste(city, first_year, " - ", last_year))

```

```{r Period and ref data monthly, eval = eval_visualization}

# Calculate period (= span years) means over the year  
# 
# assign years to periods with length = span (e.g. 15years) 
#                                starting backwards with last full year

period_adapt <- last_year %% span  # last_year mod x years => 
# last period has to be a full period, first period may be shorter 
# assign Years to Periods w/ length of span and last full period until last year 

data_monthly_period  <- as_tibble(data_monthly) %>% 
  mutate(Period_end = span*ceiling((Year - period_adapt)/span) + period_adapt) %>%
  # assign Year_Month to Periods w/ length of span with full period with last_year 
  group_by(City, Measure, Month, Period_end) %>% 
  dplyr::summarise(count = mean(count, na.rm = TRUE)) %>% 
  # mutate(Period_end = paste0(Period_end - span + 1, "-", Period_end)) %>% 
  ungroup() 

data_monthly_ref_period <- as_tibble(data_monthly) %>% 
  filter(Year >= ref_period_start & Year <= ref_period_end) %>% 
  mutate(Period_end = ref_period_end) %>%
  # assign Year_Month to Periods w/ length of span with full period with last_year 
  group_by(City, Measure, Month, Period_end) %>% 
  dplyr::summarise(count = mean(count, na.rm = TRUE)) %>% 
  mutate(Ref_Period = paste0(Period_end - span + 1, "-", Period_end)) %>% 
  ungroup() %>% 
  rename(count_ref_period = count)

data_monthly_period  <- 
  left_join(data_monthly_period, 
            data_monthly_ref_period %>% select(-Period_end), 
            by = c("City",  "Measure", "Month")) %>% 
  group_by(Month, Measure) %>% 
  mutate(count_minus_ref = count - count_ref_period) 

```

### `r span`-year period plots with monthly breakdown - Cartesian and Polar Coordinates

```{r Period and ref data yearly, eval = eval_visualization}

end_first_period <- min(data_monthly_period$Period_end)
start_last_period <- max(data_monthly_period$Period_end) - span + 1

data_yearly_period <- data_monthly_period %>%  
  mutate(Period = paste0(Period_end - span + 1, "-", Period_end)) %>%
  group_by(City, Measure, Period) %>% 
  dplyr::summarise(count = mean(count, na.rm = TRUE)) %>% 
  pivot_wider(names_from = Measure, values_from = count)

data_yearly_period$Period[1] <-  paste0(first_year, "-", end_first_period)

data_yearly_ref_period <- data_monthly_ref_period %>%  
  mutate(Period = paste0(Period_end - span + 1, "-", Period_end)) %>%
  group_by(City, Measure, Ref_Period) %>% 
  dplyr::summarise(count = mean(count_ref_period, na.rm = TRUE)) 

data_yearly  <- 
  left_join(data_yearly, data_yearly_ref_period %>% select(-Ref_Period), 
            by = c("City",  "Measure")) %>% 
  rename(count_ref_period = count)

data_yearly_ref_period <- data_yearly_ref_period %>% 
  pivot_wider(names_from = Measure, values_from = count)

# cat("Note: First Period shorter, starts with first data year =", first_year)
caption_measure <- paste0("Average Data (", y_label_text, ")")

if ("Precipitation" %in% measure_analysis) {
  data_yearly_period <- data_yearly_period %>% 
    mutate("Annual Precipitation" = Precipitation * 12) %>% 
    rename("Monthly Precipitation" = Precipitation)
  data_yearly_ref_period <- data_yearly_ref_period %>% 
    mutate("Annual Precipitation" = Precipitation * 12) %>% 
    rename("Monthly Precipitation" = Precipitation)
}

knitr::kable(data_yearly_period, align = "c", digits = 1, 
             caption = paste0(span, "-years Periods - ", caption_measure))
knitr::kable(data_yearly_ref_period, align = "c", digits = 1)
```

Note: First Period shorter in general (starts with first data year = `r first_year`)

```{r further period settings, eval = eval_visualization}

period_years <- c(paste(first_year, "-", end_first_period, sep = ""),
          paste(ref_period_start, "-", ref_period_end, sep = ""),
          paste(start_last_period, "-", last_year, sep = ""))
names(period_years) <- c("first", "ref", "last")

```


```{r seasonal, eval = eval_visualization, fig.asp = 0.5}

title <- paste0("Monthly Variations of ", span, "-Year Periods")
subtitle <- paste("Periods: First", period_years["first"], 
                  "/ Reference", period_years["ref"],
                  "/ Last", period_years["last"])
# subtitle <- paste("First / Reference / Last Period:",
#                         paste(period_years, collapse = " / "))
      
y_label_delta <- 
  expression(paste("Delta ", CO[2], " Concentration / ppm to First Period"))

# !!  coord_polar doesn't support free scales => generate separate plots
# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
    data_filter <- filter(data_monthly_period, Measure == i)
    plot_monthly <- ggts_year_over_month(data_filter, period = Period_end) +
      labs(y = y_label_measure[i], title = paste(title, "-", i), subtitle = subtitle)
    ratio_display <- 4 # ratio x / y
    ratio_values <- 12 /
      (max(data_filter$count)-min(data_filter$count))
    ratio <- ratio_values / ratio_display
    print(plot_monthly) # + coord_fixed(ratio = ratio))
    print(plot_monthly + coord_polar("x", start = pi))  # pi => rotate by 180 degrees
  }
  # }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```

### Plot Monthly Delta to Reference Period - Cartesian and Polar Coordinates

```{r seasonal adjusted, eval = eval_visualization, fig.asp = 0.5}

# !!  coord_polar doesn't support free scales => generate separate plots
# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
    data_filter <- filter(data_monthly_period, Measure == i)
    plot_monthly <- ggts_year_over_month(data_filter, period = Period_end,
                                         y = count_minus_ref) +
      labs(y = y_label_measure[i], 
           title = paste0(title, " - Delta to Reference"), 
           subtitle = subtitle) 

    print(plot_monthly) # + coord_fixed(ratio = ratio)) 
    print(plot_monthly + coord_polar("x", start = pi)) 
    # pi => rotate by 180 degrees
  }
  #  }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```

```{r seasonal plots fabletools, eval = eval_visualization, fig.asp = 0.8 * fig_asp_mult}

data_monthly %>% gg_season(count, labels = "none") +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free") +
  labs(x = "Month", y = y_label,
       title = paste("Yearly Seasonal Plots - Monthly", topic))


data_monthly %>%
  gg_subseries(count) + 
  labs(x = "Month", y = y_label,
       title = paste("Monthly Subseries Plots - ", topic)) 

```


## Yearly  `r topic`

### Plot Yearly  `r measure`


```{r yearly, eval = eval_visualization, fig.asp = 0.8 * fig_asp_mult}

# plot mean temperature & precipitation with running/rolling mean

## Multiply "Precipitation" values/month by 12 to get mm/Year

#  England data and running mean: The red line is a 21-point binomial filter, 
#                      which is roughly equivalent to a 10-year running mean.
# for Global Temperature Circle see:
# https://www.metoffice.gov.uk/weather/climate-change/what-is-climate-change
# 
# span of years of moving/rolling average/mean (einfacher Gleitender Durchschnitt) 
# default: method = "convolution", sides = 2 => centred) 

# Gauss-Filterung ; Filtergewichte proportional zur Gauss’schen Normalverteilung
# default: method = "convolution", sides = 2 => centred, circular = FALSE)
gauss <- function(x, sig=1) { 1/sqrt(2*pi)/sig * exp(-(x^2)/2/sig) }
x <- seq(from = -(span-1)/2, to = (span-1)/2, by = 1)
# x <- seq(from = -span, to = span, by = 1) 
sig <- span
filter_coef <- gauss(x, sig)/sum(gauss(x, sig))  # normalization 
# sum(filter_coef)  # has to be 1

# Hanning-Window/Von-Hann-Fenster Filter Paket: ‘e1071’ 
# similar gauss/bell-shaped curve
# see also:  https://de.wikipedia.org/wiki/Fensterfunktion
# plot(filter_coef, col = "red")  # gauss
# library(e1071)
# filter_coef <- hanning.window(span)/sum(hanning.window(span))
# points(hanning.window(span)/sum(hanning.window(span)), col = "blue")

# rol_mean will be calculated by ggts_w_rol_mean(); gauss() not required
# yearly_plot_data <- data_yearly %>%   
#   mutate(rol_mean = 
#            stats::filter(Year_avg, filter = rep(1/span, span)),
#          rol_gauss = 
#            stats::filter(Year_avg, filter = filter_coef)) %>% 
#   ungroup()
# 
# yearly_plot_data

data_yearly_tbl <- as_tibble(data_yearly) %>% group_by(Measure, City) 
# to avoid rolling mean over Precip - Temp
plot_yearly <- ggts_w_rol_mean(data_yearly_tbl, Year, Year_avg, span = span) +
  geom_line(aes(y=count_ref_period),  linetype = "dashed", size = 0.7) +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free")

plot_yearly +
  labs(y = y_label,
       title = paste0("Yearly Data with ", span, 
                      "−years Rolling Mean and Regression lines"),
       subtitle = paste0("Reference Period ", 
                         ref_period_start, "-", ref_period_end, 
                         ": black dashed line"))


# * Blue:    Annual Mean Temperature / Precipitation
# * Red:     Rolling mean with span of `r span` years
# * Orange:  Rolling gaussian filtering with span of `r span` years
# * Green:   Linear Regression line
# * Darkblue: Local Polynomial Regression Fitting (Loess)
# * Black:    Reference Period `r period_years["ref"]`: Annual Mean Temperature / Precipitation

```

### Plot Seasonal Yearly `r measure`

```{r season, eval = eval_visualization, fig.asp = 0.8 * fig_asp_mult}
# 1)  seasonal values are calculated from the monthly data by averaging the three monthly values
# 2)  Seasons are Dec-Feb (DJF), Mar-May (MAM), June-Aug (JJA), Sept-Nov (SON).
# 3)  Winter: year refers to January.
# 
# 4)  Note for  ENGLAND TEMPERATURE data:  Seasonal data exist for Spring 1659 onwards..

ggts_season(data = data_yearly) +
  facet_wrap(vars(Measure), ncol = 1,  scales = "free") + 
                                              # vars(!!!key(data_yearly))
  labs(y = y_label)

```
`

# Trend and Seasonal Analysis

## Time Series Decomposition - Trend and Seasonal Components

An *additive model* would be used when the variations around the trend do not vary
with the level of the time series whereas a *multiplicative model* would be
appropriate if the trend is proportional to the level of the time series.

Time series using an

* additive model: $y_t = T_t + C_t + S_t + \epsilon_t$  

* multiplicative model: $y_t = T_t * C_t * S_t * \epsilon_t$ 

Trend / Cycle / Seasonal / Noise component  
Cyclical components is often grouped into the Trend component

For *Seasonal decomposition of time series by Loess (stlplus)* uses in general
an additive error modle, it only provides facilities for additive decompositions.
It is possible to obtain a multiplicative decomposition by first taking logs of 
the data.


```{r decomposition replace NAs, eval = eval_trend_season, fig.asp = 1.2}
## decomposition

# filter(data_monthly, Measure == "Temperature")
# filter(data_monthly, Measure == "Precipitation")

# first_year <- data_monthly %$% min(year(Year_Month))
# last_year <- data_monthly %$% max(year(Year_Month))
# data_monthly <- data_monthly %>%  dplyr::select(Year_Month, count)



# only one single time series for ts() and stl() allowed

#   data_ts <- data_monthly %$% ts(count, start=c(first_year, 1), frequency = freq)
#   # t.window default: nextodd of
#   # s.window <- c(5, 7, 13, 21, 31, 61, 121, 241, 1001, 2001)
#   # (ceiling((1.5*freq) / (1-(1.5/s.window))))
#   #    s.window:   5, 7, 13, 21, 31, 61, 121    
#   # => t.window:  27 23, 21, 21, 19, 19,  19  #
#   #                        min = 19 da ceiling(1.5*12)=18 => nextodd = 19
   

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # Measure
    data_monthly_new <- tibble()  
      data_ts <- filter(data_monthly, Measure == i) %$% 
        ts(count, start=c(first_year, 1), frequency = freq)
      stlplus_data <- stlplus(data_ts, s.window=season_window, t.window=trend_window)
      
      data_monthly_stlplus <- uts_stlplus_as_tibble(stlplus_data)  %>% 
        mutate(Measure = i)
      # data_monthly_new <- bind_rows(data_monthly_stlplus, data_monthly_new) 
      
      decomp_plot <- ggts_decomp(data_monthly_stlplus %>% filter(Year >= 2019))
      print(decomp_plot + 
              labs(title =  paste("Seasonal Decomposition by Loess - ", i),
                   subtitle = paste("by stlplus(); Data = Trend + Seasonal + Remainder; ",
                                    "Horiz. Lines: Mean values"),
                   y = y_label_measure[i]))
    } 
    ## 'count' now NAs are replaced by values of Interpolated !!  #################
    # data_monthly <- data_monthly_new %>% 
    #   mutate(Year_Month = yearmonth(Year_Month),  # lost yearmonth() class by bind_row
    #          City = city) %>%  # lost yearmonth() by bind_rows
    #   as_tsibble(index = Year_Month, key = key)
#  }  
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```



```{r decomp with fabletools, eval = FALSE, fig.asp = 1.2}
## decomposition - needs selected keys
# *Seasonal decomposition of time series by Loess (STL)*
  
data_monthly %>% 
  # filter(year(Year_Month) >=2000) %>% 
  model(STL(count ~ trend(window=trend_window) + season(window=season_window))) %>% 
  components() %>%
  autoplot() + theme(legend.position = "bottom") + xlab("Year")
  # facet_grid(vars(!!!key(data_monthly)), scales = "free")


```




## Periodicities - Season Frequency  

### Lag Plot - Differences 

```{r lag with single filter, eval = eval_trend_season}

## !! for data with more than one time series
## => filter a single time series to use `gg_lag()` or `gg_tsdisplay()`
# 
# print(n_keys(data_monthly))   # number of separate time series
# print(key_data(data_monthly)) # tibble with key name(s) and subnames
# #     Measure       .rows        
# #   <chr>         <list>       
# # 1 Precipitation <int [1,596]>
# # 2 Temperature   <int [1,596]>
# print(key(data_monthly))  # provides "group" key name;
#                              for facet_wrap use: vars(!!!key(data_monthly))

year_filter <- 2000

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) { 
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {     # City
  #  for (j in unique(key_data(data_monthly)[[2]])) {   # Measure
      
      plot_lag <- filter(data_monthly, year(Year_Month) >= year_filter & 
                           Measure == i) %>%   # City == i &Measure == j
        gg_lag(count, lags = 1:12, geom = "path", arrow = TRUE) +
        labs(x = y_label_measure[i], y = "lag(x, n)",
             col = "Month",
             title ="Lag by n months - y(t) plotted against y(t-n)",
             subtitle = paste(i, " - as of year", year_filter)) 
print(plot_lag)
    }
#  }
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```

### ACF / PACF Correlogram

```{r ACF PACF plots, eval = FALSE}
# Plots covered by next chunk - Triplet 
# tsdisplay plot

## ACF Correlogram
plot_1 <- data_monthly %>% ACF(count, lag_max = 24) %>% 
  autoplot() + 
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1, scales = "free", 
             strip.position = "left") +
  ggtitle("ACF Correlogram - w/ slow decrease as the lags increase", 
          subtitle = "- due to the trend, while the 'scalloped' shape is due the seasonality") +
  labs(y = paste("ACF ", "bounds = 1.96/sqrt(nrow(data)) = ",
                 round(2/sqrt(nrow(data)), digits = 3)))

## PACF - Partial ACF
plot_2 <- data_monthly %>% PACF(count, lag_max = 24) %>% 
  autoplot() +
  facet_wrap(vars(!!!key(data_monthly)), ncol = 1,  scales = "free",
             strip.position = "left") +
  ggtitle("PACF - Partial Autocorrelation Function")

gridExtra::grid.arrange(plot_1, plot_2)
```


### Periodogram - Spectral Density Estimation of a Time Series

The spectral density characterizes the frequency content of the signal.
One purpose of estimating the spectral density is to detect any periodicities 
in the data, by observing peaks at the frequencies corresponding to these periodicities.

At frequency $\lambda = 1/12$ there is a significant peak
=> This pattern repeats every full frequency = every 12 months / every year

The remaining peaks are random and therefore cannot be assigned significantly.

```{r Triplet tsdisplay plot, eval = eval_trend_season}
# , error = TRUE
# 
## !! for data with more than one time series
## => filter a single time series to use `gg_lag()` or `gg_tsdisplay()`

## arttention: for Cottbus Plot with Spectrum throws an error =>error = TRUE !!
## - up to know only for Cottbus Precipitation (w/ Temperature running) 
## - Fehler in .$spec[, 1] : falsche Anzahl von Dimensionen

year_filter <- 2000

# if (n_keys(data_monthly) == 1 | length(key(data_monthly)) == 2) {
if (length(key(data_monthly)) == 1) { 
  for (i in unique(key_data(data_monthly)[[1]])) {
   # for (j in unique(key_data(data_monthly)[[2]])) {
      plot_ts_spec <- data_monthly %>% 
        filter(year(Year_Month) >= year_filter & Measure == i) %>%  
        gg_tsdisplay(count,  lag_max = 24) +  # plot_type = "spectrum",
        labs( x = "Year", y = y_label_measure[i], 
              title = "Time series plot with ACF and Spectrum",
              subtitle = paste(i, " - as of year", year_filter))
      print(plot_ts_spec)
      
      plot_ts_pacf <- data_monthly %>% 
        filter(year(Year_Month) >= year_filter & Measure == i) %>%  
        gg_tsdisplay(count, plot_type = "partial", lag_max = 24) + 
        labs( x = "Year", y = y_label_measure[i], 
              title = "Time series plot with ACF and PACF",
              subtitle = paste(i, " - as of year", year_filter))
      print(plot_ts_pacf)
    }
#  } 
} else {
  abort(paste("nothing implemented for n_keys/key_data/key(data_monthly) =", 
              n_keys(data_monthly), key_data(data_monthly), (key(data_monthly))))
}

```


### Seasonal vs non Seasonal ACF / Strength (Seasonal/Trend)

* Check acf1 and season_acf1 and compare with ACF Correlogram Plot
* acf1: first autocorrelation coefficient from the original data
* acf10: sum of square of the first ten autocorrelation coefficients from the original data
* diff1_acf1: first autocorrelation coefficient from the differenced data
* season_acf1: autocorrelation coefficient at the first seasonal lag

* Check Trend & Seasonal Strength close to 0 / 1 : weak / strong and compare them
* stl_e_acf1: first autocorrelation coefficient of the remainder series
* stl_e_acf10: sum of squares of the first ten autocorrelation coefficients of the remainder series
* linearity: linearity of the trend component of the STL decomposition. It is based on the coefficient of a linear regression applied to the trend component
* curvature: curvature of the trend component of the STL decomposition. It is based on the coefficient from an orthogonal quadratic regression applied to the trend component.


```{r seasonal vs non seasonal, eval = eval_trend_season}

print("Check acf1 and season_acf1 and compare with ACF Correlogram Plot")
data_monthly %>% features(count, feat_acf)

print("Check Trend & Seasonal Strength close to 0 / 1 : weak / strong and compare them")
data_monthly %>% features(count, feat_stl)

plot_1 <- data_monthly %>% features(count, feat_acf) %>%
  ggplot(aes(x=acf1, y=season_acf1, col=Measure)) +
  geom_point() + facet_wrap(vars(Measure)) +
  theme(legend.position = "none") +
  ggtitle("Seasonal ACF lag1 vs. ACF lag1")
plot_2 <- data_monthly %>% features(count, feat_stl) %>%
  ggplot(aes(x=trend_strength, y=seasonal_strength_year, col=Measure)) +
  geom_point() + facet_wrap(vars(Measure)) +
  theme(legend.position = "none") +
  ggtitle("Seasonal vs. Trend Strength")
gridExtra::grid.arrange(plot_1, plot_2)


```


### Spectral Entropy Test

* Entropy close to 0 => series has strong trend and seasonality (=> easy to forecast)
* Entropy close to 1 => series is very noisy (and so is difficult to forecast) 

```{r Entropy test, eval = eval_trend_season}
# (Shannon) spectral entropy of a time series, which is a measure of how easy 
# the series is to forecast. 
# A series which has strong trend and seasonality (and so is easy to forecast) 
# will have entropy close to 0. 
# A series that is very noisy (and so is difficult to forecast) 
# will have entropy close to 1.

print("Check entropy close to 0 or 1")
data_monthly %>% features(count, feat_spectral) 
# entropy close to 0 => series has strong trend and seasonality (=> easy to forecast)
# entropy close to 1 => series is very noisy (and so is difficult to forecast)

```


## Stationary Process Test

Strict-sense stationarity / Weak (wide-sense) stationarity

Augmented Dickey-Fuller test => type3, a linear model with both drift and linear trend

Trend Stationary - underlying trend (function solely of time) can be removed, 
leaving a stationary process

```{r eval = FALSE}
## Stationarity Test
library(aTSA)     # for stationary.test(data_ts) and adf.test(data_ts) - same
library(fractal)  # for stationaryty()
library(LaplacesDemon)

first_year<- data_monthly %$% year(min((Year_Month)))
# first_year <- as.integer(as_tibble(data_monthly) %>% 
#                            summarise(first_year = min(Year)))
freq <- 12
# data <- data_monthly_stlplus  # w/ numeric "Time"
# as.numeric(as.Date("2018-09-24") - as.Date("2017-10-24"))
# update data_ts with replaced NAs, e.g. adf.test() does not allow NAs
data_ts <- ts(data_monthly$count, start=c(first_year, 1), frequency = freq)

# Performs stationary test for a univariate time series
# combines the existing functions adf.test, pp.test and kpss.test 
# for testing the stationarity of a univariate time series x.
# null hypothesis of a unit root of a univarate time series x 
# (equivalently, x is a non-stationary time series)
# => p > 0.05 h0 can not be rejected  => non-stationary
# cat("Null Hypothesis of non-stationary time series - for p < 0.05: reject H_0")
# stationary.test(data_ts) 

# already covered by unitroot_kpss test
# data_monthly %>% 
#   features(count, list(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))

# The results are the same as one of the adf.test, pp.test, kpss.test, depending on which test 
# for MAuna Loa: adf.test: 

# adf.test(data_ts)  # Augmented Dickey-Fuller test

# kpss.test(data_ts) # KPSS Unit Root Test 
# pp.test(data_ts)   # Phillips-Perron Unit Root Test

# Priestley-Subba Rao (PSR) test
statio <-
  stationarity(data_monthly$count, n.taper = 6,
               n.block = max(c(12, floor(logb(length(data_ts), base = 12)))),
               significance = 0.05, center = TRUE, recenter = FALSE)
summary(statio)

is.stationary(data_monthly$count)
# statcheck() from Schlittgen
source("./Data_Schlittgen/tsutil.R")
statcheck(data_monthly$count, 5) #Berechnung der deskriptiven Maßzahlen ist elementar. 
     # Um die Kovarianzstationarität zu überprüfen, wird die Funktion eingesetzt
     #  plot ACF over Lag  for 5 segments
#  function statcheck determines the means, standard deviations and acf’s
#  of segments of a time series and plots the acf’s for the segments.

# already covered by unitroot_ndiffs, unitroot_nsdiffs tests
# data_monthly %>% 
#   features(count, list(unitroot_kpss, unitroot_ndiffs, unitroot_nsdiffs))
# vartable(data_ts,12) # variate Differenzen weisen darauf hin, dass einmal 
#                      # einfache und einmal saisonale Differenzen zu bilden sind
#                      # sind, um Stationarität zu erreichen.



# bandfilt(data_ts, 7, 12, 24)
# View(data_ts)
# 
# (model_lm_1 <- lm(count ~ Year_Month, data))
# summary(model_lm_1)
# plot(model_lm_1)
# 
# model_lm_2 <- lm(count ~ Year_Month + Year_Month^2, data)
# summary(model_lm_2)
# plot(model_lm_2)


```


```{r Beispiel 10.10, eval = FALSE}
library(dlm)                                                                                 
# co2 <- scan("./Data_Schlittgen/schauinsland.dat")  
co2 <- data_ts  # co2 <- ts(co2,start = 2005,frequency=12)
dlmco <- dlmModPoly(2) + dlmModSeas(12)
m1 <- c(382,0.1,rep(0,11))  
c1 <- diag(c( 0.1, 0.1, rep(100,11)))

buildFun <- function(x) { 
  W(dlmco)[1:3,1:3] <- diag(exp(x[1:3]))
  V(dlmco) <- exp(x[4]) 
  C0(dlmco) <- c1
  m0(dlmco) <- m1 
  return(dlmco)
}              

# Parameter estimation by maximum likelihood - very long runnning !!
fit <- dlmMLE(co2, parm=rep(1,4), build=buildFun)
fit$conv
dlm.co2 <- buildFun(fit$par)                                              
coFilter <- dlmFilter(co2, mod=dlm.co2)
coSmooth <- dlmSmooth(coFilter) 

plot(co2,type="o") 
lines(dropFirst(coFilter$m[,1]))
lines(dropFirst(coSmooth$s[,1]))
plot(dropFirst(coSmooth$s[,3]))

fut1 <- dlmForecast(coFilter, n=12)
x <- cbind(co2,fut1$f,fut1$f-1.96*sqrt(unlist(fut1$Q)),
           fut1$f+1.96*sqrt(unlist(fut1$Q)))                                
plot.ts(x,plot.type="s",lty=c(1,1,2,2)) 
points(2005+28/12,co2[29])




```


# Backup

## `r city` - Average Yearly and Seasonal Data

```{r data print, eval = eval_backup}

data_yearly_temp <- bind_rows(as_tibble(slice(data_yearly, 1:10)), 
                              as_tibble(slice(data_yearly, n() -9:0))) %>% 
  select(City, Measure, Year, 
         Winter_avg, Spring_avg, Summer_avg, Fall_avg, Year_avg )

# knitr printout in for loop not running, print(knitr()) destroys formatting
i <- 1
knitr::kable(filter(data_yearly_temp, Measure == measure_analysis[i]),
             digits = 1, caption = 
               paste("Annual", y_label_measure[measure_analysis[i]], 
                     "(first and last 10 years)"))

# in if loop knitr printout only for last knitr() statement
if (length(measure_analysis) >= 2) {
  i <- 2
  knitr::kable(filter(data_yearly_temp, Measure == measure_analysis[i]),
               digits = 1, caption = 
                 paste("Annual", y_label_measure[measure_analysis[i]], 
                       "(first and last 10 years)"))
}

# in if loop knitr printout only for last knitr() statement
if (length(measure_analysis) == 3) {
  i <- 3
  knitr::kable(filter(data_yearly_temp, Measure == measure_analysis[i]),
               digits = 1, caption = 
                 paste("Annual", y_label_measure[measure_analysis[i]], 
                       "(first and last 10 years)"))
}


mean_monthly_Wide <- mean_monthly %>% 
  pivot_wider(names_from = Measure, values_from = Month_avg) 

knitr::kable(filter(mean_monthly_Wide),
             digits = 1, caption = 
               paste0("Monthly Means over all Years (", y_label_text, ")"))
# paste(y_label_measure, collapse = ", ")))

```

## Data Sources 


### Temperatures and Precipitation

* Basel / Davos: **Federal Office of Meteorology and Climatology MeteoSwiss**

<https://www.meteoswiss.admin.ch/home/climate/swiss-climate-in-detail/homogeneous-data-series-since-1864.html>

* `r paste(city_dwd, collapse = "/ ")`: **DWD Archiv Monats- und Tageswerte**

<https://www.dwd.de/DE/leistungen/klimadatendeutschland/klarchivtagmonat.html>  

(*Monatswerte historisch und aktuell*, 
column MO_TT (Temperature; Monatsmittel der Lufttemperatur in 2m Höhe in 
\textdegree C and	MO_RR (Precipitation; Monatssumme der Niederschlagshoehe in mm))

* England **Met Office - National Meteorological Service for the UK**

<https://www.metoffice.gov.uk/hadobs/hadcet/data/download.html>
Monthly_HadCET_mean.txt, 1659 to date


### CO2 Concentrations

**National Oceanic & Atmospheric Administration - Earth System Research Laboratory**

*NOAA ESRL *<https://www.esrl.noaa.gov/gmd/ccgg/trends/global.html>

Data file: Mauna Loa CO2 monthly mean data

<https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html>


## R code

Partially based on *c't Magazin* articles by *Andreas Krause*:    
#3/2014 p.188 <http://www.ct.de/1403188> & #6/2014 p.180 <http://www.ct.de/1406180> 




```{r ACF with ggplot, eval = FALSE}

## Test Plots

# acf() does not allow NAs and facetting
data_ts <- ts(data_monthly$count, start=c(first_year, 1), frequency = freq)

# Korrelogramm
# „empirische Autokovarianz- und Autokorrelationsfunktion (ACF)
# 
#  Lag - Verschiebung, des betrachteten Zeitfensters
#  Lag 0: Jan mit Jan (sich selbst)
#  Lag 1: Jan mit Feb (next Feb , einen Monat weiter)
#  
#  Lag 12: Jan mit Jan (des nächsten Jahres)
#  t <- c(1,2,3) => lag(t): [1] NA  1  2
lag_max <- 4 * freq
data_acf <- acf(data_ts, lag.max = lag_max, plot = FALSE, type = "correlation")
data_pacf <- acf(data_ts, lag.max = lag_max, plot = FALSE, type = "partial")
# plot(data_acf)
 
# Temperaturzeitreihe weist natürliche saisonale Schwankungen auf
# => gut im Korellogramm zu erkennen kann
# Der Januar wird als Basis gewählt und die Monate um den Januar herum weisen 
#   eine sehr hohe positive Korrelation auf
# die Sommermonate in der Mitte (0,5) des Jahres weisen eine starke
#                  negative Korrelation auf
data_acf_tbl <- tibble(ACF = c(NA, as.vector(data_acf$acf[-1])), 
    # take out redundant first value lag = 0 => always one (also taken out by ACF()) !
                       PACF = c(NA, as.vector(data_pacf$acf)), # has no lag=0 value !
                       Lag = as.vector(freq*data_acf$lag))
data_acf_tbl %<>% pivot_longer(
  cols = c("ACF", "PACF"),
  names_to = "ACF_PACF",
  values_to = "value") 
bounds <- round(2/sqrt(data_acf$n.used), digits = 3)  # data_acf$n.used = nrow(data)
ggplot(data_acf_tbl, aes(Lag, value)) +
  geom_bar(stat = "identity", width = 0.2) +  
  # geom_line() +
  geom_hline(yintercept = bounds, col = "blue", lty = "dashed") +
  geom_hline(yintercept = -bounds, col = "blue", lty = "dashed") +  
  facet_wrap( ~ ACF_PACF, ncol = 1, scales = "free", strip.position = "left") +
  ggtitle(paste("Time Series Correlogram"),
          subtitle = paste("with Season Frequency = ", freq, ",  lag.max = ", lag_max)) +
  labs(y = "", x = "Lag / months")

  

```


```{r eval = FALSE}
# Periodogramm
# Das Periodogramm ist das Pendant zum Korellogramm auf Spektralebene
# Es ist die abgebildete Fouriertransformation der Autokovarianzfunktion 
data_period <- 
  spec.pgram(data_ts, taper = 0, pad = 0, fast = TRUE, demean = FALSE,
             detrend = TRUE, plot = FALSE, na.action = na.fail)
#  Frequenz λ = 1: signiﬁkanter Ausschlag stattﬁndet
#  => vorliegende Muster wiederholt sich jede volle Frequenz / jedes Jahr
#  Die restlichen Ausschläge sind bereits zu zufällig, um sie signiﬁkant 
#                                                    einordnen zu können.

data_period_tbl <- as_tibble(data_period$freq) %>% 
  mutate(Frequency = value,
         Spectrum = data_period$spec)

ggplot(data_period_tbl, aes(Frequency, Spectrum)) +
  geom_line() +
  # geom_point() +
  labs(x = paste("Frequency\n", "bandwidth = ", data_period$bandwidth)) +
  scale_y_log10() +
  ggtitle( paste("Time Series", data_period$method))

```



```{r yearly data with replaced NAs, eval = FALSE}

### now with replaced NAs

data_monthly_wide <- data_monthly %>%
  as_tibble() %>% 
  pivot_wider(id_cols = c(Year, Measure),
              names_from = Month,
              values_from = count) %>% 
  mutate(City = city)

# data_yearly - generation from data_monthly_wide
#     data_check_and_fill_w_na(data_monthly,
# w/o gaps and as tsibble (no winter, ... since season = FALSE), maybe w/ NAs
# 
data_yearly <- uts_gen_yearly_seasonal_avg(data_monthly)
data_yearly <- as_tsibble(data_yearly, index = Year, key = key)


nr_series_w_gaps <- ifelse(has_gaps(data_yearly, .full = TRUE)$.gaps, 1, 0) 

if (sum(nr_series_w_gaps) > 0) {    # Check for missing years
  print(data_yearly %>% scan_gaps(.full = TRUE))  # Reveal
  print(data_yearly %>% count_gaps(.full = TRUE)) # Summarise - number of gaps from - to
  data_yearly %<>% fill_gaps(.full = TRUE)       # Fill in time gaps
  
  print(data_yearly %>% has_gaps(.full = TRUE))   # Check for gaps
}
  
```

```{r end, echo = FALSE}
###########################################################################

Prog.End <- Sys.time()
run.time <- round((Prog.End - Prog.Start), digits = 2)
message("Program executed on: ", Prog.End, "\t Program run time: ", run.time, " secs")

```
